/*
**
** builtins.h
**
** generated by compiler version "8.1.10.4"
**
** Copyright (c) 1997-2011 Analog Devices, Inc.
**
*/

#ifdef __VERSIONNUM__
#if __VERSIONNUM__ != 0x08010a04
#error The compiler version does not match the version of the builtins.h include
#endif
#endif

#pragma system_header /*builtins.h*/

#include <sys/builtins_support.h>

#include <fract_typedef.h>
#include <fr2x16_typedef.h>

#ifndef _ACC40_TYPEDEF_H
#define _ACC40_TYPEDEF_H
typedef long long acc40;
#endif

#ifdef _MISRA_RULES
#pragma diag(push)
#pragma diag(suppress:misra_rule_2_4)
#pragma diag(suppress:misra_rule_5_1:"Allow identifiers longer than 31 characters")
#pragma diag(suppress:misra_rule_5_3)
#pragma diag(suppress:misra_rule_6_3)
#pragma diag(suppress:misra_rule_8_1)
#pragma diag(suppress:misra_rule_8_8)
#pragma diag(suppress:misra_rule_8_5)
#pragma diag(suppress:misra_rule_19_7)
#pragma diag(suppress:misra_rule_19_15)
#pragma diag(suppress:misra_rule_20_2)
#endif

#if !defined(_SIZE_T) && !defined(_SIZET) && \
    !defined(__SIZE_T_DEFINED)
#define _SIZE_T
#define _SIZET
#define __SIZE_T_DEFINED
typedef long unsigned int size_t;
#endif

#if !defined(_PTRDIFF_T) && !defined(_PTRDIFFT) && \
    !defined(__PTRDIFF_T_DEFINED)
#define _PTRDIFF_T
#define _PTRDIFFT
#define __PTRDIFF_T_DEFINED
typedef long int ptrdiff_t;
#endif

#ifdef __cplusplus
extern "C" {
#endif

#if !defined(__NO_BUILTIN)

void * __builtin_memcpy(void * __a, const void * __b, size_t  __c);
void * __builtin_memmove(void * __a, const void * __b, size_t  __c);
char * __builtin_strcpy(char * __a, const char * __b);
size_t  __builtin_strlen(const char * __a);
void * __builtin_va_start(const void * __a, int  __b);
int  __builtin_abs(int  __a);
long  __builtin_labs(long  __a);
int  __builtin_expected_true(int  __a);
int  __builtin_expected_false(int  __a);
void __builtin_assert(int  __a);
void __builtin_aligned(const void * __a, int  __b);
int  __builtin_funcsize(const void * __a);
ptrdiff_t  __builtin_circindex(ptrdiff_t  __a, ptrdiff_t  __b, size_t  __c);
void * __builtin_circptr(const void * __a, ptrdiff_t  __b, const void * __c, size_t  __d);
short  __builtin_misaligned_load16(void * __a);
short  __builtin_misaligned_load16_vol(volatile void * __a);
void __builtin_misaligned_store16(void * __a, short  __b);
void __builtin_misaligned_store16_vol(volatile void * __a, short  __b);
int  __builtin_misaligned_load32(void * __a);
int  __builtin_misaligned_load32_vol(volatile void * __a);
void __builtin_misaligned_store32(void * __a, int  __b);
void __builtin_misaligned_store32_vol(volatile void * __a, int  __b);
long long  __builtin_misaligned_load64(void * __a);
long long  __builtin_misaligned_load64_vol(volatile void * __a);
void __builtin_misaligned_store64(void * __a, long long  __b);
void __builtin_misaligned_store64_vol(volatile void * __a, long long  __b);
int  __builtin_max(int  __a, int  __b);
long  __builtin_lmax(long  __a, long  __b);
int  __builtin_min(int  __a, int  __b);
long  __builtin_lmin(long  __a, long  __b);
void * __builtin_alloca(size_t  __a);
void * __builtin_dealloca(size_t  __a);
fract16  __builtin_sum_fr2x16(fract2x16  __a);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_sum_fx_fr2x16(fract2x16  __a);
#endif
fract32  __builtin_mult_fr1x32x32(fract32  __a, fract32  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_mult_fx1x32x32(long _Fract  __a, long _Fract  __b);
#endif
fract32  __builtin_multr_fr1x32x32(fract32  __a, fract32  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_multr_fx1x32x32(long _Fract  __a, long _Fract  __b);
#endif
fract32  __builtin_mult_fr1x32x32NS(fract32  __a, fract32  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_mult_fx1x32x32NS(long _Fract  __a, long _Fract  __b);
#endif
int  __builtin_byteswap4(int  __a);
short  __builtin_byteswap2(short  __a);
short  __builtin_sum_i2x16(int  __a);
int  __builtin_cmplx_mac(int  __a, int  __b, int  __c);
int  __builtin_cmplx_msu(int  __a, int  __b, int  __c);
int  __builtin_cmplx_mac_s40(int  __a, int  __b, int  __c);
int  __builtin_cmplx_msu_s40(int  __a, int  __b, int  __c);
int  __builtin_cmplx_mul_s40(int  __a, int  __b);
int  __builtin_cmplx_conj_mac(int  __a, int  __b, int  __c);
int  __builtin_cmplx_conj_msu(int  __a, int  __b, int  __c);
int  __builtin_cmplx_conj_mac_s40(int  __a, int  __b, int  __c);
int  __builtin_cmplx_conj_msu_s40(int  __a, int  __b, int  __c);
int  __builtin_cmplx_conj_mul_s40(int  __a, int  __b);
int  __builtin_csqu_fr16(int  __a);
fract16  __builtin_diff_hl_fr2x16(fract2x16  __a);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_diff_hl_fx_fr2x16(fract2x16  __a);
#endif
fract16  __builtin_diff_lh_fr2x16(fract2x16  __a);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_diff_lh_fx_fr2x16(fract2x16  __a);
#endif
long long  __builtin_cadd_fr32(long long  __a, long long  __b);
long long  __builtin_csub_fr32(long long  __a, long long  __b);
long long  __builtin_conj_fr32(long long  __a);
long long  __builtin_cmplx_mul32(long long  __a, long long  __b);
fract16  __builtin_shr_fr1x16(fract16  __a, short  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_shr_fx1x16(_Fract  __a, short  __b);
#endif
fract32  __builtin_shr_fr1x32(fract32  __a, short  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_shr_fx1x32(long _Fract  __a, short  __b);
#endif
fract2x16  __builtin_shr_fr2x16(fract2x16  __a, short  __b);
short  __builtin_shr_i1x16(short  __a, short  __b);
fract2x16  __builtin_shrl_fr2x16(fract2x16  __a, short  __b);
fract32  __builtin_shr_fr1x32_clip(fract32  __a, short  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_shr_fx1x32_clip(long _Fract  __a, short  __b);
#endif
fract2x16  __builtin_shr_fr2x16_clip(fract2x16  __a, short  __b);
fract2x16  __builtin_shrl_fr2x16_clip(fract2x16  __a, short  __b);
fract32  __builtin_shl_fr1x32_clip(fract32  __a, short  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_shl_fx1x32_clip(long _Fract  __a, short  __b);
#endif
fract2x16  __builtin_shl_fr2x16_clip(fract2x16  __a, short  __b);
long long  __builtin_mult64_32x32(int  __a, int  __b);
unsigned long long  __builtin_multu64_32x32(unsigned int  __a, unsigned int  __b);
int  __builtin_addclip_lo(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_addclip_hi(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_addclip_lor(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_addclip_hir(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_extract_and_add_r1(int  __a, int  __b);
int  __builtin_extract_and_add_r2(int  __a);
int  __builtin_add_i4x8_r1(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_add_i4x8_r2(int  __a);
int  __builtin_add_i4x8_r_r1(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_add_i4x8_r_r2(int  __a);
int  __builtin_avg_i4x8(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_avg_i4x8_t(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_avg_i4x8_r(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_avg_i4x8_tr(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_avg_i2x8_lo(long long  __a, char * __b, long long  __c);
int  __builtin_avg_i2x8_lot(long long  __a, char * __b, long long  __c);
int  __builtin_avg_i2x8_lor(long long  __a, char * __b, long long  __c);
int  __builtin_avg_i2x8_lotr(long long  __a, char * __b, long long  __c);
int  __builtin_avg_i2x8_hi(long long  __a, char * __b, long long  __c);
int  __builtin_avg_i2x8_hit(long long  __a, char * __b, long long  __c);
int  __builtin_avg_i2x8_hir(long long  __a, char * __b, long long  __c);
int  __builtin_avg_i2x8_hitr(long long  __a, char * __b, long long  __c);
int  __builtin_sub_i4x8_r1(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_sub_i4x8_r2(int  __a);
int  __builtin_sub_i4x8_r_r1(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_sub_i4x8_r_r2(int  __a);
int  __builtin_saa_r1(long long  __a, char * __b, long long  __c, char * __d, int  __e, int  __f);
int  __builtin_saa_r2(int  __a);
int  __builtin_saa_r_r1(long long  __a, char * __b, long long  __c, char * __d, int  __e, int  __f);
int  __builtin_saa_r_r2(int  __a);
int  __builtin_byteunpack_r1(long long  __a, char * __b);
int  __builtin_byteunpack_r2(int  __a);
int  __builtin_byteunpackr_r1(long long  __a, char * __b);
int  __builtin_byteunpackr_r2(int  __a);
long long  __builtin_bitmux_shr_r1(long long  __a, int  __b, int  __c);
int  __builtin_bitmux_shr_r2(long long  __a);
int  __builtin_bitmux_shr_r3(long long  __a);
long long  __builtin_bitmux_shl_r1(long long  __a, int  __b, int  __c);
int  __builtin_bitmux_shl_r2(long long  __a);
int  __builtin_bitmux_shl_r3(long long  __a);
short  __builtin_A_bxorshift_mask32_r1(acc40  __a, int  __b);
int  __builtin_A_bxorshift_mask32_r2(short  __a);
short  __builtin_A_bxor_mask32_r1(acc40  __a, int  __b);
int  __builtin_A_bxor_mask32_r2(short  __a);
acc40  __builtin_A_bxorshift_mask40(acc40  __a, acc40  __b, int  __c);
short  __builtin_A_bxor_mask40_r1(acc40  __a, acc40  __b, int  __c);
int  __builtin_A_bxor_mask40_r2(short  __a);
void  __builtin_untestset(char * __a);
unsigned long long  __builtin_sysreg_read64(int  __a);
void  __builtin_sysreg_write64(int  __a, unsigned long long  __b);
void  __builtin_csync(void);
void  __builtin_ssync(void);
unsigned int  __builtin_sysreg_read(int  __a);
void  __builtin_sysreg_write(int  __a, unsigned int  __b);
unsigned long long  __builtin_emuclk(void);
acc40  __builtin_A_sat(acc40  __a);
fract32  __builtin_A_mad(acc40  __a);
fract32  __builtin_A_mad_FU(acc40  __a);
fract32  __builtin_A_mad_S2RND(acc40  __a);
int  __builtin_A_mad_ISS2(acc40  __a);
short  __builtin_A_madh_IS(acc40  __a);
unsigned short  __builtin_A_madh_IU(acc40  __a);
fract16  __builtin_A_madh(acc40  __a);
fract16  __builtin_A_madh_FU(acc40  __a);
fract16  __builtin_A_madh_T(acc40  __a);
fract16  __builtin_A_madh_TFU(acc40  __a);
fract16  __builtin_A_madh_S2RND(acc40  __a);
short  __builtin_A_madh_ISS2(acc40  __a);
short  __builtin_A_madh_IH(acc40  __a);
int  __builtin_A_eq(acc40  __a, acc40  __b);
int  __builtin_A_lt(acc40  __a, acc40  __b);
int  __builtin_A_le(acc40  __a, acc40  __b);
short  __builtin_ones(int  __a);
int  __builtin_A_bitmux_ASL_r1(int  __a, int  __b, acc40  __c);
int  __builtin_A_bitmux_ASL_r2(int  __a);
acc40  __builtin_A_bitmux_ASL_r3(int  __a);
int  __builtin_A_bitmux_ASR_r1(int  __a, int  __b, acc40  __c);
int  __builtin_A_bitmux_ASR_r2(int  __a);
acc40  __builtin_A_bitmux_ASR_r3(int  __a);
short  __builtin_shl_i1x16(short  __a, short  __b);
int  __builtin_shl_i2x16(int  __a, short  __b);
fract16  __builtin_shl_fr1x16(fract16  __a, short  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_shl_fx1x16(_Fract  __a, short  __b);
#endif
fract2x16  __builtin_shl_fr2x16(fract2x16  __a, short  __b);
int  __builtin_shl_i1x32(int  __a, short  __b);
fract32  __builtin_shl_fr1x32(fract32  __a, short  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_shl_fx1x32(long _Fract  __a, short  __b);
#endif
acc40  __builtin_A_ashift(acc40  __a, short  __b);
short  __builtin_shll_i1x16(short  __a, short  __b);
int  __builtin_shll_i2x16(int  __a, short  __b);
int  __builtin_shll_i1x32(int  __a, short  __b);
acc40  __builtin_A_lshift(acc40  __a, short  __b);
fract32  __builtin_abs_fr1x32(fract32  __a);
int  __builtin_abs_i1x32(int  __a);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_abs_fx1x32(long _Fract  __a);
#endif
acc40  __builtin_A_abs(acc40  __a);
fract2x16  __builtin_abs_fr2x16(fract2x16  __a);
int  __builtin_abs_i2x16(int  __a);
acc40  __builtin_A_sub(acc40  __a, acc40  __b);
acc40  __builtin_A_add(acc40  __a, acc40  __b);
void * __builtin_brevadd(void * __a, void * __b);
int  __builtin_divs_r1(int  __a, int  __b);
int  __builtin_divs_r2(int  __a);
int  __builtin_divq_r1(int  __a, int  __b, int  __c);
int  __builtin_divq_r2(int  __a);
short  __builtin_expadj1x32(int  __a, short  __b);
short  __builtin_expadj1x16(short  __a, short  __b);
short  __builtin_expadj2x16(int  __a, short  __b);
short  __builtin_lvitmax1x16_r1(int  __a, int  __b);
int  __builtin_lvitmax1x16_r2(short  __a);
short  __builtin_rvitmax1x16_r1(int  __a, int  __b);
int  __builtin_rvitmax1x16_r2(short  __a);
int  __builtin_lvitmax2x16_r1(int  __a, int  __b, int  __c);
int  __builtin_lvitmax2x16_r2(int  __a);
int  __builtin_rvitmax2x16_r1(int  __a, int  __b, int  __c);
int  __builtin_rvitmax2x16_r2(int  __a);
int  __builtin_max_i2x16(int  __a, int  __b);
fract2x16  __builtin_max_fr2x16(fract2x16  __a, fract2x16  __b);
fract2x16  __builtin_min_fr2x16(fract2x16  __a, fract2x16  __b);
int  __builtin_min_i2x16(int  __a, int  __b);
fract32  __builtin_negate_fr1x32(fract32  __a);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_negate_fx1x32(long _Fract  __a);
#endif
acc40  __builtin_A_neg(acc40  __a);
fract2x16  __builtin_negate_fr2x16(fract2x16  __a);
fract16  __builtin_round_fr1x32(fract32  __a);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_round_fx1x32(long _Fract  __a);
#endif
short  __builtin_norm_fr1x32(fract32  __a);
#ifdef __FIXED_POINT_ALLOWED
short  __builtin_norm_fx1x32(long _Fract  __a);
#endif
short  __builtin_norm_fr1x16(fract16  __a);
#ifdef __FIXED_POINT_ALLOWED
short  __builtin_norm_fx1x16(_Fract  __a);
#endif
short  __builtin_A_signbits(acc40  __a);
fract16  __builtin_multr_fr1x16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_multr_fx1x16(_Fract  __a, _Fract  __b);
#endif
fract16  __builtin_multmr_fr1x16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_multmr_fx1x16(_Fract  __a, unsigned _Fract  __b);
#endif
fract16  __builtin_multm_fr1x16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_multm_fx1x16(_Fract  __a, unsigned _Fract  __b);
#endif
fract2x16  __builtin_cmplx_mul(fract2x16  __a, fract2x16  __b);
fract2x16  __builtin_cmplx_conj_mul(fract2x16  __a, fract2x16  __b);
short  __builtin_csqumag_fr16(int  __a);
int  __builtin_csqumag_fr32(int  __a);
acc40  __builtin_A_mult_IS(short  __a, short  __b);
acc40  __builtin_A_mac_IS(acc40  __a, short  __b, short  __c);
acc40  __builtin_A_msu_IS(acc40  __a, short  __b, short  __c);
acc40  __builtin_A_mult_MIS(short  __a, unsigned short  __b);
acc40  __builtin_A_mac_MIS(acc40  __a, short  __b, unsigned short  __c);
acc40  __builtin_A_msu_MIS(acc40  __a, short  __b, unsigned short  __c);
acc40  __builtin_A_mult(fract16  __a, fract16  __b);
acc40  __builtin_A_mac(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_A_msu(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_A_mult_FU(fract16  __a, fract16  __b);
acc40  __builtin_A_mac_FU(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_A_msu_FU(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_A_mult_M(fract16  __a, fract16  __b);
acc40  __builtin_A_mac_M(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_A_msu_M(acc40  __a, fract16  __b, fract16  __c);
int  __builtin_add_i2x16(int  __a, int  __b);
unsigned int  __builtin_add_u2x16(unsigned int  __a, unsigned int  __b);
fract2x16  __builtin_add_fr2x16(fract2x16  __a, fract2x16  __b);
int  __builtin_cmplx_add(int  __a, int  __b);
int  __builtin_sub_i2x16(int  __a, int  __b);
unsigned int  __builtin_sub_u2x16(unsigned int  __a, unsigned int  __b);
fract2x16  __builtin_sub_fr2x16(fract2x16  __a, fract2x16  __b);
int  __builtin_cmplx_sub(int  __a, int  __b);
fract2x16  __builtin_dspaddsubsat(fract2x16  __a, fract2x16  __b);
fract2x16  __builtin_dspsubaddsat(fract2x16  __a, fract2x16  __b);
int  __builtin_add_on_sign(int  __a, int  __b);
int * __builtin_search_gt_r1(int  __a, int * __b, int * __c, int * __d, short  __e, short  __f);
int * __builtin_search_gt_r2(int * __a);
short  __builtin_search_gt_r3(int * __a);
short  __builtin_search_gt_r4(int * __a);
int * __builtin_search_ge_r1(int  __a, int * __b, int * __c, int * __d, short  __e, short  __f);
int * __builtin_search_ge_r2(int * __a);
short  __builtin_search_ge_r3(int * __a);
short  __builtin_search_ge_r4(int * __a);
int * __builtin_search_le_r1(int  __a, int * __b, int * __c, int * __d, short  __e, short  __f);
int * __builtin_search_le_r2(int * __a);
short  __builtin_search_le_r3(int * __a);
short  __builtin_search_le_r4(int * __a);
int * __builtin_search_lt_r1(int  __a, int * __b, int * __c, int * __d, short  __e, short  __f);
int * __builtin_search_lt_r2(int * __a);
short  __builtin_search_lt_r3(int * __a);
short  __builtin_search_lt_r4(int * __a);
fract2x16  __builtin_multr_fr2x16(fract2x16  __a, fract2x16  __b);
int  __builtin_mult_i2x16(int  __a, int  __b);
fract2x16  __builtin_mult_fr2x16(fract2x16  __a, fract2x16  __b);
int  __builtin_multu_fr2x16(int  __a, int  __b);
int  __builtin_align8(int  __a, int  __b);
int  __builtin_align16(int  __a, int  __b);
int  __builtin_align24(int  __a, int  __b);
int  __builtin_loadbytes(int * __a);
int  __builtin_bytepack(int  __a, int  __b);
void  __builtin_NOP(void);
unsigned int  __builtin_cli(void);
void  __builtin_sti(unsigned int  __a);
void  __builtin_csync_int(void);
void  __builtin_ssync_int(void);
void  __builtin_idle(void);
void  __builtin_halt(void);
void  __builtin_abort(void);
void  __builtin_raise(int  __a);
void  __builtin_excpt(int  __a);
int  __builtin_testset(char * __a);
void  __builtin_prefetch(void * __a);
void * __builtin_prefetchmodup(void * __a);
void  __builtin_flushinv(void * __a);
void * __builtin_flushinvmodup(void * __a);
void  __builtin_flush(void * __a);
void * __builtin_flushmodup(void * __a);
void  __builtin_iflush(void * __a);
void * __builtin_iflushmodup(void * __a);
void * __builtin_bfin_iflushmodup(void * __a);
void  __builtin_bfin_iflush(void * __a);
void * __builtin_bfin_flushmodup(void * __a);
void  __builtin_bfin_flush(void * __a);
void * __builtin_bfin_flushinvmodup(void * __a);
void  __builtin_bfin_flushinv(void * __a);
void * __builtin_bfin_prefetchmodup(void * __a);
void  __builtin_bfin_prefetch(void * __a);
int  __builtin_bfin_testset(char * __a);
void  __builtin_bfin_excpt(int  __a);
void  __builtin_bfin_raise(int  __a);
void  __builtin_bfin_abort(void);
void  __builtin_bfin_halt(void);
void  __builtin_bfin_idle(void);
void  __builtin_bfin_ssync_int(void);
void  __builtin_bfin_csync_int(void);
void  __builtin_bfin_sti(unsigned int  __a);
unsigned int  __builtin_bfin_cli(void);
int  __builtin_bfin_bytepack(int  __a, int  __b);
int  __builtin_bfin_loadbytes(int * __a);
int  __builtin_bfin_align24(int  __a, int  __b);
int  __builtin_bfin_align16(int  __a, int  __b);
int  __builtin_bfin_align8(int  __a, int  __b);
int  __builtin_bfin_multu_fr2x16(int  __a, int  __b);
fract2x16  __builtin_bfin_mult_fr2x16(fract2x16  __a, fract2x16  __b);
int  __builtin_bfin_mult_i2x16(int  __a, int  __b);
fract2x16  __builtin_bfin_multr_fr2x16(fract2x16  __a, fract2x16  __b);
fract2x16  __builtin_bfin_dspsubaddsat(fract2x16  __a, fract2x16  __b);
fract2x16  __builtin_bfin_dspaddsubsat(fract2x16  __a, fract2x16  __b);
fract2x16  __builtin_bfin_sub_fr2x16(fract2x16  __a, fract2x16  __b);
int  __builtin_bfin_sub_i2x16(int  __a, int  __b);
fract2x16  __builtin_bfin_add_fr2x16(fract2x16  __a, fract2x16  __b);
int  __builtin_bfin_add_i2x16(int  __a, int  __b);
acc40  __builtin_bfin_A_msu_M(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_bfin_A_mac_M(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_bfin_A_mult_M(fract16  __a, fract16  __b);
acc40  __builtin_bfin_A_msu_FU(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_bfin_A_mac_FU(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_bfin_A_mult_FU(fract16  __a, fract16  __b);
acc40  __builtin_bfin_A_msu(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_bfin_A_mac(acc40  __a, fract16  __b, fract16  __c);
acc40  __builtin_bfin_A_mult(fract16  __a, fract16  __b);
acc40  __builtin_bfin_A_msu_MIS(acc40  __a, short  __b, unsigned short  __c);
acc40  __builtin_bfin_A_mac_MIS(acc40  __a, short  __b, unsigned short  __c);
acc40  __builtin_bfin_A_mult_MIS(short  __a, unsigned short  __b);
acc40  __builtin_bfin_A_msu_IS(acc40  __a, short  __b, short  __c);
acc40  __builtin_bfin_A_mac_IS(acc40  __a, short  __b, short  __c);
acc40  __builtin_bfin_A_mult_IS(short  __a, short  __b);
int  __builtin_bfin_csqumag_fr32(int  __a);
short  __builtin_bfin_csqumag_fr16(int  __a);
fract2x16  __builtin_bfin_cmplx_conj_mul(fract2x16  __a, fract2x16  __b);
fract2x16  __builtin_bfin_cmplx_mul(fract2x16  __a, fract2x16  __b);
fract16  __builtin_bfin_multm_fr1x16(fract16  __a, fract16  __b);
fract16  __builtin_bfin_multmr_fr1x16(fract16  __a, fract16  __b);
fract16  __builtin_bfin_multr_fr1x16(fract16  __a, fract16  __b);
short  __builtin_bfin_A_signbits(acc40  __a);
short  __builtin_bfin_norm_fr1x16(fract16  __a);
short  __builtin_bfin_norm_fr1x32(fract32  __a);
fract16  __builtin_bfin_round_fr1x32(fract32  __a);
fract2x16  __builtin_bfin_negate_fr2x16(fract2x16  __a);
acc40  __builtin_bfin_A_neg(acc40  __a);
fract32  __builtin_bfin_negate_fr1x32(fract32  __a);
fract2x16  __builtin_bfin_min_fr2x16(fract2x16  __a, fract2x16  __b);
int  __builtin_bfin_max_i2x16(int  __a, int  __b);
int  __builtin_bfin_rvitmax2x16_r1(int  __a, int  __b, int  __c);
int  __builtin_rvitmax2x16_r2(int  __a);
int  __builtin_bfin_lvitmax2x16_r1(int  __a, int  __b, int  __c);
int  __builtin_lvitmax2x16_r2(int  __a);
short  __builtin_bfin_rvitmax1x16_r1(int  __a, int  __b);
int  __builtin_rvitmax1x16_r2(short  __a);
short  __builtin_bfin_lvitmax1x16_r1(int  __a, int  __b);
int  __builtin_lvitmax1x16_r2(short  __a);
short  __builtin_bfin_expadj2x16(int  __a, short  __b);
short  __builtin_bfin_expadj1x16(short  __a, short  __b);
short  __builtin_bfin_expadj1x32(int  __a, short  __b);
int  __builtin_bfin_divq_r1(int  __a, int  __b, int  __c);
int  __builtin_divq_r2(int  __a);
int  __builtin_bfin_divs_r1(int  __a, int  __b);
int  __builtin_divs_r2(int  __a);
acc40  __builtin_bfin_A_add(acc40  __a, acc40  __b);
acc40  __builtin_bfin_A_sub(acc40  __a, acc40  __b);
fract2x16  __builtin_bfin_abs_fr2x16(fract2x16  __a);
acc40  __builtin_bfin_A_abs(acc40  __a);
fract32  __builtin_bfin_abs_fr1x32(fract32  __a);
acc40  __builtin_bfin_A_lshift(acc40  __a, short  __b);
int  __builtin_bfin_shll_i1x32(int  __a, short  __b);
int  __builtin_bfin_shll_i2x16(int  __a, short  __b);
short  __builtin_bfin_shll_i1x16(short  __a, short  __b);
acc40  __builtin_bfin_A_ashift(acc40  __a, short  __b);
fract32  __builtin_bfin_shl_fr1x32(fract32  __a, short  __b);
int  __builtin_bfin_shl_i1x32(int  __a, short  __b);
fract2x16  __builtin_bfin_shl_fr2x16(fract2x16  __a, short  __b);
fract16  __builtin_bfin_shl_fr1x16(fract16  __a, short  __b);
int  __builtin_bfin_shl_i2x16(int  __a, short  __b);
short  __builtin_bfin_shl_i1x16(short  __a, short  __b);
int  __builtin_bfin_A_bitmux_ASR_r1(int  __a, int  __b, acc40  __c);
int  __builtin_A_bitmux_ASR_r2(int  __a);
acc40  __builtin_A_bitmux_ASR_r3(int  __a);
int  __builtin_bfin_A_bitmux_ASL_r1(int  __a, int  __b, acc40  __c);
int  __builtin_A_bitmux_ASL_r2(int  __a);
acc40  __builtin_A_bitmux_ASL_r3(int  __a);
short  __builtin_bfin_ones(int  __a);
int  __builtin_bfin_A_le(acc40  __a, acc40  __b);
int  __builtin_bfin_A_lt(acc40  __a, acc40  __b);
int  __builtin_bfin_A_eq(acc40  __a, acc40  __b);
short  __builtin_bfin_A_madh_IH(acc40  __a);
short  __builtin_bfin_A_madh_ISS2(acc40  __a);
fract16  __builtin_bfin_A_madh_S2RND(acc40  __a);
fract16  __builtin_bfin_A_madh_TFU(acc40  __a);
fract16  __builtin_bfin_A_madh_T(acc40  __a);
fract16  __builtin_bfin_A_madh_FU(acc40  __a);
fract16  __builtin_bfin_A_madh(acc40  __a);
unsigned short  __builtin_bfin_A_madh_IU(acc40  __a);
short  __builtin_bfin_A_madh_IS(acc40  __a);
int  __builtin_bfin_A_mad_ISS2(acc40  __a);
fract32  __builtin_bfin_A_mad_S2RND(acc40  __a);
fract32  __builtin_bfin_A_mad_FU(acc40  __a);
fract32  __builtin_bfin_A_mad(acc40  __a);
acc40  __builtin_bfin_A_sat(acc40  __a);
unsigned long long  __builtin_bfin_emuclk(void);
void  __builtin_bfin_sysreg_write(int  __a, unsigned int  __b);
unsigned int  __builtin_bfin_sysreg_read(int  __a);
void  __builtin_bfin_ssync(void);
void  __builtin_bfin_csync(void);
void  __builtin_bfin_sysreg_write64(int  __a, unsigned long long  __b);
unsigned long long  __builtin_bfin_sysreg_read64(int  __a);
void  __builtin_bfin_untestset(char * __a);
long long  __builtin_bfin_bitmux_shl_r1(long long  __a, int  __b, int  __c);
int  __builtin_bitmux_shl_r2(long long  __a);
int  __builtin_bitmux_shl_r3(long long  __a);
long long  __builtin_bfin_bitmux_shr_r1(long long  __a, int  __b, int  __c);
int  __builtin_bitmux_shr_r2(long long  __a);
int  __builtin_bitmux_shr_r3(long long  __a);
int  __builtin_bfin_byteunpackr_r1(long long  __a, char * __b);
int  __builtin_byteunpackr_r2(int  __a);
int  __builtin_bfin_byteunpack_r1(long long  __a, char * __b);
int  __builtin_byteunpack_r2(int  __a);
int  __builtin_bfin_saa_r_r1(long long  __a, char * __b, long long  __c, char * __d, int  __e, int  __f);
int  __builtin_saa_r_r2(int  __a);
int  __builtin_bfin_saa_r1(long long  __a, char * __b, long long  __c, char * __d, int  __e, int  __f);
int  __builtin_saa_r2(int  __a);
int  __builtin_bfin_sub_i4x8_r_r1(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_sub_i4x8_r_r2(int  __a);
int  __builtin_bfin_sub_i4x8_r1(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_sub_i4x8_r2(int  __a);
int  __builtin_bfin_avg_i2x8_hitr(long long  __a, char * __b, long long  __c);
int  __builtin_bfin_avg_i2x8_hir(long long  __a, char * __b, long long  __c);
int  __builtin_bfin_avg_i2x8_hit(long long  __a, char * __b, long long  __c);
int  __builtin_bfin_avg_i2x8_hi(long long  __a, char * __b, long long  __c);
int  __builtin_bfin_avg_i2x8_lotr(long long  __a, char * __b, long long  __c);
int  __builtin_bfin_avg_i2x8_lor(long long  __a, char * __b, long long  __c);
int  __builtin_bfin_avg_i2x8_lot(long long  __a, char * __b, long long  __c);
int  __builtin_bfin_avg_i2x8_lo(long long  __a, char * __b, long long  __c);
int  __builtin_bfin_avg_i4x8_tr(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_bfin_avg_i4x8_r(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_bfin_avg_i4x8_t(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_bfin_avg_i4x8(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_bfin_add_i4x8_r_r1(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_add_i4x8_r_r2(int  __a);
int  __builtin_bfin_add_i4x8_r1(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_add_i4x8_r2(int  __a);
int  __builtin_bfin_extract_and_add_r1(int  __a, int  __b);
int  __builtin_extract_and_add_r2(int  __a);
int  __builtin_bfin_addclip_hir(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_bfin_addclip_lor(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_bfin_addclip_hi(long long  __a, char * __b, long long  __c, char * __d);
int  __builtin_bfin_addclip_lo(long long  __a, char * __b, long long  __c, char * __d);
unsigned long long  __builtin_bfin_multu64_32x32(unsigned int  __a, unsigned int  __b);
long long  __builtin_bfin_mult64_32x32(int  __a, int  __b);
fract2x16  __builtin_bfin_shl_fr2x16_clip(fract2x16  __a, short  __b);
fract32  __builtin_bfin_shl_fr1x32_clip(fract32  __a, short  __b);
fract2x16  __builtin_bfin_shrl_fr2x16_clip(fract2x16  __a, short  __b);
fract2x16  __builtin_bfin_shr_fr2x16_clip(fract2x16  __a, short  __b);
fract32  __builtin_bfin_shr_fr1x32_clip(fract32  __a, short  __b);
fract2x16  __builtin_bfin_shrl_fr2x16(fract2x16  __a, short  __b);
short  __builtin_bfin_shr_i1x16(short  __a, short  __b);
fract2x16  __builtin_bfin_shr_fr2x16(fract2x16  __a, short  __b);
fract32  __builtin_bfin_shr_fr1x32(fract32  __a, short  __b);
fract16  __builtin_bfin_shr_fr1x16(fract16  __a, short  __b);
long long  __builtin_bfin_cmplx_mul32(long long  __a, long long  __b);
long long  __builtin_bfin_conj_fr32(long long  __a);
long long  __builtin_bfin_csub_fr32(long long  __a, long long  __b);
long long  __builtin_bfin_cadd_fr32(long long  __a, long long  __b);
fract16  __builtin_bfin_diff_lh_fr2x16(fract2x16  __a);
fract16  __builtin_bfin_diff_hl_fr2x16(fract2x16  __a);
int  __builtin_bfin_csqu_fr16(int  __a);
int  __builtin_bfin_cmplx_conj_mul_s40(int  __a, int  __b);
int  __builtin_bfin_cmplx_conj_msu_s40(int  __a, int  __b, int  __c);
int  __builtin_bfin_cmplx_conj_mac_s40(int  __a, int  __b, int  __c);
int  __builtin_bfin_cmplx_conj_msu(int  __a, int  __b, int  __c);
int  __builtin_bfin_cmplx_conj_mac(int  __a, int  __b, int  __c);
int  __builtin_bfin_cmplx_mul_s40(int  __a, int  __b);
int  __builtin_bfin_cmplx_msu_s40(int  __a, int  __b, int  __c);
int  __builtin_bfin_cmplx_mac_s40(int  __a, int  __b, int  __c);
int  __builtin_bfin_cmplx_msu(int  __a, int  __b, int  __c);
int  __builtin_bfin_cmplx_mac(int  __a, int  __b, int  __c);
short  __builtin_bfin_sum_i2x16(int  __a);
short  __builtin_bfin_byteswap2(short  __a);
int  __builtin_bfin_byteswap4(int  __a);
fract32  __builtin_bfin_mult_fr1x32x32NS(fract32  __a, fract32  __b);
fract32  __builtin_bfin_multr_fr1x32x32(fract32  __a, fract32  __b);
fract32  __builtin_bfin_mult_fr1x32x32(fract32  __a, fract32  __b);
fract16  __builtin_bfin_sum_fr2x16(fract2x16  __a);
int  __builtin_set_rnd_mod_biased(void);
int  __builtin_set_rnd_mod_unbiased(void);
void  __builtin_restore_rnd_mod(int  __a);
fract16  __builtin_sat_fr1x32(fract32  __a);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_sat_fx1x32(long _Fract  __a);
#endif
int  __builtin_sat_fr1x64(long long  __a);
fract16  __builtin_mult_fr1x16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_mult_fx1x16(_Fract  __a, _Fract  __b);
#endif
fract16  __builtin_multu_fr1x16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
unsigned _Fract  __builtin_multu_fx1x16(unsigned _Fract  __a, unsigned _Fract  __b);
#endif
fract16  __builtin_add_fr1x16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_add_fx1x16(_Fract  __a, _Fract  __b);
#endif
fract16  __builtin_sub_fr1x16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_sub_fx1x16(_Fract  __a, _Fract  __b);
#endif
fract16  __builtin_min_fr1x16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_min_fx1x16(_Fract  __a, _Fract  __b);
#endif
fract16  __builtin_max_fr1x16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_max_fx1x16(_Fract  __a, _Fract  __b);
#endif
fract32  __builtin_add_fr1x32(fract32  __a, fract32  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_add_fx1x32(long _Fract  __a, long _Fract  __b);
#endif
fract32  __builtin_sub_fr1x32(fract32  __a, fract32  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_sub_fx1x32(long _Fract  __a, long _Fract  __b);
#endif
fract32  __builtin_mult_fr1x32(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_mult_fx1x32(_Fract  __a, _Fract  __b);
#endif
fract32  __builtin_min_fr1x32(fract32  __a, fract32  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_min_fx1x32(long _Fract  __a, long _Fract  __b);
#endif
fract32  __builtin_max_fr1x32(fract32  __a, fract32  __b);
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_max_fx1x32(long _Fract  __a, long _Fract  __b);
#endif
int  __builtin_compose_2x16(short  __a, short  __b);
int  __builtin_ccompose_fr16(fract16  __a, fract16  __b);
#ifdef __FIXED_POINT_ALLOWED
int  __builtin_ccompose_fx_fr16(_Fract  __a, _Fract  __b);
#endif
long long  __builtin_compose_i64(int  __a, int  __b);
short  __builtin_extract_hi(int  __a);
fract16  __builtin_imag_fr16(int  __a);
fract32  __builtin_imag_fr32(long long  __a);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_imag_fx_fr16(int  __a);
#endif
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_imag_fx_fr32(long long  __a);
#endif
short  __builtin_extract_lo(int  __a);
fract16  __builtin_real_fr16(int  __a);
fract32  __builtin_real_fr32(long long  __a);
#ifdef __FIXED_POINT_ALLOWED
_Fract  __builtin_real_fx_fr16(int  __a);
#endif
#ifdef __FIXED_POINT_ALLOWED
long _Fract  __builtin_real_fx_fr32(long long  __a);
#endif
void __builtin_untestset(char * __a);
unsigned short  __builtin_mmr_read16(volatile void * __a);
unsigned int  __builtin_mmr_read32(volatile void * __a);
void __builtin_mmr_write16(volatile void * __a, unsigned short  __b);
void __builtin_mmr_write32(volatile void * __a, unsigned int  __b);

#endif /* __NO_BUILTIN */


#if !defined(__NO_BUILTIN) && !defined(__NO_SHORTNAMES)

#if (!defined(__DEFINED_MEMCPY) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MEMCPY)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MEMCPY))))

#define __DEFINED_MEMCPY

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * memcpy(void * __a, const void * __b, size_t  __c) {
  void * __rval = __builtin_memcpy(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_MEMCPY */

#if (!defined(__DEFINED_MEMMOVE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MEMMOVE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MEMMOVE))))

#define __DEFINED_MEMMOVE

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * memmove(void * __a, const void * __b, size_t  __c) {
  void * __rval = __builtin_memmove(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_MEMMOVE */

#if (!defined(__DEFINED_STRCPY) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_STRCPY)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_STRCPY))))

#define __DEFINED_STRCPY

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static char * strcpy(char * __a, const char * __b) {
  char * __rval = __builtin_strcpy(__a, __b);
  return __rval;
}

#endif /* __DEFINED_STRCPY */

#if (!defined(__DEFINED_STRLEN) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_STRLEN)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_STRLEN))))

#define __DEFINED_STRLEN

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static size_t  strlen(const char * __a) {
  size_t  __rval = __builtin_strlen(__a);
  return __rval;
}

#endif /* __DEFINED_STRLEN */

#if (!defined(__DEFINED_VA_START) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_VA_START)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_VA_START))))

#define __DEFINED_VA_START

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * va_start(const void * __a, int  __b) {
  void * __rval = __builtin_va_start(__a, __b);
  return __rval;
}

#endif /* __DEFINED_VA_START */

#if (!defined(__DEFINED_ABS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ABS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ABS))))

#define __DEFINED_ABS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  abs(int  __a) {
  int  __rval = __builtin_abs(__a);
  return __rval;
}

#endif /* __DEFINED_ABS */

#if (!defined(__DEFINED_LABS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_LABS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_LABS))))

#define __DEFINED_LABS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long  labs(long  __a) {
  long  __rval = __builtin_labs(__a);
  return __rval;
}

#endif /* __DEFINED_LABS */

#if (!defined(__DEFINED_EXPECTED_TRUE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EXPECTED_TRUE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EXPECTED_TRUE))))

#define __DEFINED_EXPECTED_TRUE

#define expected_true(A) (__builtin_expected_true((A)))

#endif /* __DEFINED_EXPECTED_TRUE */

#if (!defined(__DEFINED_EXPECTED_FALSE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EXPECTED_FALSE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EXPECTED_FALSE))))

#define __DEFINED_EXPECTED_FALSE

#define expected_false(A) (__builtin_expected_false((A)))

#endif /* __DEFINED_EXPECTED_FALSE */

#if (!defined(__DEFINED_ASSERT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ASSERT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ASSERT))))

#define __DEFINED_ASSERT

#define assert(A) (__builtin_assert((A)))

#endif /* __DEFINED_ASSERT */

#if (!defined(__DEFINED_ALIGNED) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ALIGNED)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ALIGNED))))

#define __DEFINED_ALIGNED

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void aligned(const void * __a, int  __b) {
  __builtin_aligned(__a, __b);
}

#endif /* __DEFINED_ALIGNED */

#if (!defined(__DEFINED_FUNCSIZE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_FUNCSIZE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_FUNCSIZE))))

#define __DEFINED_FUNCSIZE

#define funcsize(A) (__builtin_funcsize((A)))

#endif /* __DEFINED_FUNCSIZE */

#if (!defined(__DEFINED_CIRCINDEX) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CIRCINDEX)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CIRCINDEX))))

#define __DEFINED_CIRCINDEX

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static ptrdiff_t  circindex(ptrdiff_t  __a, ptrdiff_t  __b, size_t  __c) {
  ptrdiff_t  __rval = __builtin_circindex(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_CIRCINDEX */

#if (!defined(__DEFINED_CIRCPTR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CIRCPTR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CIRCPTR))))

#define __DEFINED_CIRCPTR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * circptr(const void * __a, ptrdiff_t  __b, const void * __c, size_t  __d) {
  void * __rval = __builtin_circptr(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_CIRCPTR */

#if (!defined(__DEFINED_MISALIGNED_LOAD16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_LOAD16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_LOAD16))))

#define __DEFINED_MISALIGNED_LOAD16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  misaligned_load16(void * __a) {
  short  __rval = __builtin_misaligned_load16(__a);
  return __rval;
}

#endif /* __DEFINED_MISALIGNED_LOAD16 */

#if (!defined(__DEFINED_MISALIGNED_LOAD16_VOL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_LOAD16_VOL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_LOAD16_VOL))))

#define __DEFINED_MISALIGNED_LOAD16_VOL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  misaligned_load16_vol(volatile void * __a) {
  short  __rval = __builtin_misaligned_load16_vol(__a);
  return __rval;
}

#endif /* __DEFINED_MISALIGNED_LOAD16_VOL */

#if (!defined(__DEFINED_MISALIGNED_STORE16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_STORE16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_STORE16))))

#define __DEFINED_MISALIGNED_STORE16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void misaligned_store16(void * __a, short  __b) {
  __builtin_misaligned_store16(__a, __b);
}

#endif /* __DEFINED_MISALIGNED_STORE16 */

#if (!defined(__DEFINED_MISALIGNED_STORE16_VOL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_STORE16_VOL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_STORE16_VOL))))

#define __DEFINED_MISALIGNED_STORE16_VOL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void misaligned_store16_vol(volatile void * __a, short  __b) {
  __builtin_misaligned_store16_vol(__a, __b);
}

#endif /* __DEFINED_MISALIGNED_STORE16_VOL */

#if (!defined(__DEFINED_MISALIGNED_LOAD32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_LOAD32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_LOAD32))))

#define __DEFINED_MISALIGNED_LOAD32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  misaligned_load32(void * __a) {
  int  __rval = __builtin_misaligned_load32(__a);
  return __rval;
}

#endif /* __DEFINED_MISALIGNED_LOAD32 */

#if (!defined(__DEFINED_MISALIGNED_LOAD32_VOL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_LOAD32_VOL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_LOAD32_VOL))))

#define __DEFINED_MISALIGNED_LOAD32_VOL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  misaligned_load32_vol(volatile void * __a) {
  int  __rval = __builtin_misaligned_load32_vol(__a);
  return __rval;
}

#endif /* __DEFINED_MISALIGNED_LOAD32_VOL */

#if (!defined(__DEFINED_MISALIGNED_STORE32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_STORE32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_STORE32))))

#define __DEFINED_MISALIGNED_STORE32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void misaligned_store32(void * __a, int  __b) {
  __builtin_misaligned_store32(__a, __b);
}

#endif /* __DEFINED_MISALIGNED_STORE32 */

#if (!defined(__DEFINED_MISALIGNED_STORE32_VOL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_STORE32_VOL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_STORE32_VOL))))

#define __DEFINED_MISALIGNED_STORE32_VOL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void misaligned_store32_vol(volatile void * __a, int  __b) {
  __builtin_misaligned_store32_vol(__a, __b);
}

#endif /* __DEFINED_MISALIGNED_STORE32_VOL */

#if (!defined(__DEFINED_MISALIGNED_LOAD64) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_LOAD64)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_LOAD64))))

#define __DEFINED_MISALIGNED_LOAD64

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  misaligned_load64(void * __a) {
  long long  __rval = __builtin_misaligned_load64(__a);
  return __rval;
}

#endif /* __DEFINED_MISALIGNED_LOAD64 */

#if (!defined(__DEFINED_MISALIGNED_LOAD64_VOL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_LOAD64_VOL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_LOAD64_VOL))))

#define __DEFINED_MISALIGNED_LOAD64_VOL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  misaligned_load64_vol(volatile void * __a) {
  long long  __rval = __builtin_misaligned_load64_vol(__a);
  return __rval;
}

#endif /* __DEFINED_MISALIGNED_LOAD64_VOL */

#if (!defined(__DEFINED_MISALIGNED_STORE64) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_STORE64)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_STORE64))))

#define __DEFINED_MISALIGNED_STORE64

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void misaligned_store64(void * __a, long long  __b) {
  __builtin_misaligned_store64(__a, __b);
}

#endif /* __DEFINED_MISALIGNED_STORE64 */

#if (!defined(__DEFINED_MISALIGNED_STORE64_VOL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MISALIGNED_STORE64_VOL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MISALIGNED_STORE64_VOL))))

#define __DEFINED_MISALIGNED_STORE64_VOL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void misaligned_store64_vol(volatile void * __a, long long  __b) {
  __builtin_misaligned_store64_vol(__a, __b);
}

#endif /* __DEFINED_MISALIGNED_STORE64_VOL */

#if (!defined(__DEFINED_MAX) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MAX)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MAX))))

#define __DEFINED_MAX

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  max(int  __a, int  __b) {
  int  __rval = __builtin_max(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MAX */

#if (!defined(__DEFINED_LMAX) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_LMAX)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_LMAX))))

#define __DEFINED_LMAX

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long  lmax(long  __a, long  __b) {
  long  __rval = __builtin_lmax(__a, __b);
  return __rval;
}

#endif /* __DEFINED_LMAX */

#if (!defined(__DEFINED_MIN) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MIN)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MIN))))

#define __DEFINED_MIN

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  min(int  __a, int  __b) {
  int  __rval = __builtin_min(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MIN */

#if (!defined(__DEFINED_LMIN) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_LMIN)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_LMIN))))

#define __DEFINED_LMIN

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long  lmin(long  __a, long  __b) {
  long  __rval = __builtin_lmin(__a, __b);
  return __rval;
}

#endif /* __DEFINED_LMIN */

#if (!defined(__DEFINED_ALLOCA) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ALLOCA)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ALLOCA))))

#define __DEFINED_ALLOCA

#define alloca(A) (__builtin_alloca((A)))

#endif /* __DEFINED_ALLOCA */

#if (!defined(__DEFINED_DEALLOCA) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_DEALLOCA)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_DEALLOCA))))

#define __DEFINED_DEALLOCA

#define dealloca(A) (__builtin_dealloca((A)))

#endif /* __DEFINED_DEALLOCA */

#if (!defined(__DEFINED_SUM_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUM_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUM_FR2X16))))

#define __DEFINED_SUM_FR2X16

/* Performs a saturating sideways addition of the two fract16s 
** in the input and returns the result
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  sum_fr2x16(fract2x16  __a) {
  fract16  __rval = __builtin_sum_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_SUM_FR2X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SUM_FX_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUM_FX_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUM_FX_FR2X16))))

#define __DEFINED_SUM_FX_FR2X16

/* Performs a saturating sideways addition of the two fract16s 
** in the input and returns the result
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  sum_fx_fr2x16(fract2x16  __a) {
  _Fract  __rval = __builtin_sum_fx_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_SUM_FX_FR2X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MULT_FR1X32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_FR1X32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_FR1X32X32))))

#define __DEFINED_MULT_FR1X32X32

/* Performs 32-bit fractional multiplication of the input 
** parameters. The result (which is calculated internally 
** with an accuracy of 40 bits) is rounded (biased rounding)
** to 32 bits.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  mult_fr1x32x32(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_mult_fr1x32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_FR1X32X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MULT_FX1X32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_FX1X32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_FX1X32X32))))

#define __DEFINED_MULT_FX1X32X32

/* Performs 32-bit fractional multiplication of the input 
** parameters. The result (which is calculated internally 
** with an accuracy of 40 bits) is rounded (biased rounding)
** to 32 bits.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  mult_fx1x32x32(long _Fract  __a, long _Fract  __b) {
  long _Fract  __rval = __builtin_mult_fx1x32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_FX1X32X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MULTR_FR1X32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTR_FR1X32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTR_FR1X32X32))))

#define __DEFINED_MULTR_FR1X32X32

/* Performs 32-bit fractional multiplication of the input 
** parameters with more accurate rounding than mult_fr1x32x32.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  multr_fr1x32x32(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_multr_fr1x32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTR_FR1X32X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MULTR_FX1X32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTR_FX1X32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTR_FX1X32X32))))

#define __DEFINED_MULTR_FX1X32X32

/* Performs 32-bit fractional multiplication of the input 
** parameters with more accurate rounding than mult_fx1x32x32.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  multr_fx1x32x32(long _Fract  __a, long _Fract  __b) {
  long _Fract  __rval = __builtin_multr_fx1x32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTR_FX1X32X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MULT_FR1X32X32NS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_FR1X32X32NS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_FR1X32X32NS))))

#define __DEFINED_MULT_FR1X32X32NS

/* Performs 32-bit non-saturating fractional multiplication of 
** the input parameters. Requires fewer cycles than mult_fr1x32x32.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  mult_fr1x32x32NS(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_mult_fr1x32x32NS(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_FR1X32X32NS */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MULT_FX1X32X32NS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_FX1X32X32NS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_FX1X32X32NS))))

#define __DEFINED_MULT_FX1X32X32NS

/* Performs 32-bit non-saturating fractional multiplication of 
** the input parameters. Requires fewer cycles than mult_fx1x32x32.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  mult_fx1x32x32NS(long _Fract  __a, long _Fract  __b) {
  long _Fract  __rval = __builtin_mult_fx1x32x32NS(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_FX1X32X32NS */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_BYTESWAP4) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BYTESWAP4)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BYTESWAP4))))

#define __DEFINED_BYTESWAP4

/* change data from big-endian to little-endian
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  byteswap4(int  __a) {
  int  __rval = __builtin_byteswap4(__a);
  return __rval;
}

#endif /* __DEFINED_BYTESWAP4 */

#if (!defined(__DEFINED_BYTESWAP2) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BYTESWAP2)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BYTESWAP2))))

#define __DEFINED_BYTESWAP2

/* change data from big-endian to little-endian
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  byteswap2(short  __a) {
  short  __rval = __builtin_byteswap2(__a);
  return __rval;
}

#endif /* __DEFINED_BYTESWAP2 */

#if (!defined(__DEFINED_SUM_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUM_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUM_I2X16))))

#define __DEFINED_SUM_I2X16

/* Performs a non-saturating sideways addition of the two 
** fract16s in the input and returns the result
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  sum_i2x16(int  __a) {
  short  __rval = __builtin_sum_i2x16(__a);
  return __rval;
}

#endif /* __DEFINED_SUM_I2X16 */

#if (!defined(__DEFINED_CMPLX_MAC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_MAC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_MAC))))

#define __DEFINED_CMPLX_MAC

/* complex_fract16 multiple and accumulate (__a+(__b*__c)) with 
** 32-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_mac(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_cmplx_mac(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_CMPLX_MAC */

#if (!defined(__DEFINED_CMPLX_MSU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_MSU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_MSU))))

#define __DEFINED_CMPLX_MSU

/* complex_fract16 multiple and subtract (__a-(__b*__c)) with
** 32-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_msu(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_cmplx_msu(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_CMPLX_MSU */

#if (!defined(__DEFINED_CMPLX_MAC_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_MAC_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_MAC_S40))))

#define __DEFINED_CMPLX_MAC_S40

/* complex_fract16 multiple and accumulate (__a+(__b*__c)) with 
** 40-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_mac_s40(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_cmplx_mac_s40(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_CMPLX_MAC_S40 */

#if (!defined(__DEFINED_CMPLX_MSU_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_MSU_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_MSU_S40))))

#define __DEFINED_CMPLX_MSU_S40

/* complex_fract16 multiple and subtract (__a-(__b*__c)) with
** 40-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_msu_s40(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_cmplx_msu_s40(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_CMPLX_MSU_S40 */

#if (!defined(__DEFINED_CMPLX_MUL_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_MUL_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_MUL_S40))))

#define __DEFINED_CMPLX_MUL_S40

/* complex_fract16 multiplication with 40-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_mul_s40(int  __a, int  __b) {
  int  __rval = __builtin_cmplx_mul_s40(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CMPLX_MUL_S40 */

#if (!defined(__DEFINED_CMPLX_CONJ_MAC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_CONJ_MAC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_CONJ_MAC))))

#define __DEFINED_CMPLX_CONJ_MAC

/* complex_fract16 multiply-by-conjugate and accumulate 
** (__a+(__b*__c*)) with 32-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_conj_mac(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_cmplx_conj_mac(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_CMPLX_CONJ_MAC */

#if (!defined(__DEFINED_CMPLX_CONJ_MSU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_CONJ_MSU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_CONJ_MSU))))

#define __DEFINED_CMPLX_CONJ_MSU

/* complex_fract16 multiply-by-conjugate and subtract 
** (__a-(__b*__c*)) with 32-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_conj_msu(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_cmplx_conj_msu(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_CMPLX_CONJ_MSU */

#if (!defined(__DEFINED_CMPLX_CONJ_MAC_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_CONJ_MAC_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_CONJ_MAC_S40))))

#define __DEFINED_CMPLX_CONJ_MAC_S40

/* complex_fract16 multiply-by-conjugate and accumulate 
** (__a+(__b*__c*)) with 40-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_conj_mac_s40(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_cmplx_conj_mac_s40(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_CMPLX_CONJ_MAC_S40 */

#if (!defined(__DEFINED_CMPLX_CONJ_MSU_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_CONJ_MSU_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_CONJ_MSU_S40))))

#define __DEFINED_CMPLX_CONJ_MSU_S40

/* complex_fract16 multiply-by-conjugate and subtract 
** (__a-(__b*__c*)) with 40-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_conj_msu_s40(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_cmplx_conj_msu_s40(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_CMPLX_CONJ_MSU_S40 */

#if (!defined(__DEFINED_CMPLX_CONJ_MUL_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_CONJ_MUL_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_CONJ_MUL_S40))))

#define __DEFINED_CMPLX_CONJ_MUL_S40

/* complex_fract16 multiplication by conjugate with 40-bit saturation
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_conj_mul_s40(int  __a, int  __b) {
  int  __rval = __builtin_cmplx_conj_mul_s40(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CMPLX_CONJ_MUL_S40 */

#if (!defined(__DEFINED_CSQU_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CSQU_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CSQU_FR16))))

#define __DEFINED_CSQU_FR16

/* calculates the square of the complex_fract16 input
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  csqu_fr16(int  __a) {
  int  __rval = __builtin_csqu_fr16(__a);
  return __rval;
}

#endif /* __DEFINED_CSQU_FR16 */

#if (!defined(__DEFINED_DIFF_HL_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_DIFF_HL_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_DIFF_HL_FR2X16))))

#define __DEFINED_DIFF_HL_FR2X16

/* Takes the difference (high-low) of the two fract16s in 
** the fract2x16.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  diff_hl_fr2x16(fract2x16  __a) {
  fract16  __rval = __builtin_diff_hl_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_DIFF_HL_FR2X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_DIFF_HL_FX_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_DIFF_HL_FX_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_DIFF_HL_FX_FR2X16))))

#define __DEFINED_DIFF_HL_FX_FR2X16

/* Takes the difference (high-low) of the two fract16s in 
** the fract2x16.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  diff_hl_fx_fr2x16(fract2x16  __a) {
  _Fract  __rval = __builtin_diff_hl_fx_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_DIFF_HL_FX_FR2X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_DIFF_LH_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_DIFF_LH_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_DIFF_LH_FR2X16))))

#define __DEFINED_DIFF_LH_FR2X16

/* Takes the difference (low-high) of the two fract16s in 
** the fract2x16.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  diff_lh_fr2x16(fract2x16  __a) {
  fract16  __rval = __builtin_diff_lh_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_DIFF_LH_FR2X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_DIFF_LH_FX_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_DIFF_LH_FX_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_DIFF_LH_FX_FR2X16))))

#define __DEFINED_DIFF_LH_FX_FR2X16

/* Takes the difference (low-high) of the two fract16s in 
** the fract2x16.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  diff_lh_fx_fr2x16(fract2x16  __a) {
  _Fract  __rval = __builtin_diff_lh_fx_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_DIFF_LH_FX_FR2X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_CADD_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CADD_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CADD_FR32))))

#define __DEFINED_CADD_FR32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  cadd_fr32(long long  __a, long long  __b) {
  long long  __rval = __builtin_cadd_fr32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CADD_FR32 */

#if (!defined(__DEFINED_CSUB_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CSUB_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CSUB_FR32))))

#define __DEFINED_CSUB_FR32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  csub_fr32(long long  __a, long long  __b) {
  long long  __rval = __builtin_csub_fr32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CSUB_FR32 */

#if (!defined(__DEFINED_CONJ_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CONJ_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CONJ_FR32))))

#define __DEFINED_CONJ_FR32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  conj_fr32(long long  __a) {
  long long  __rval = __builtin_conj_fr32(__a);
  return __rval;
}

#endif /* __DEFINED_CONJ_FR32 */

#if (!defined(__DEFINED_CMPLX_MUL32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_MUL32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_MUL32))))

#define __DEFINED_CMPLX_MUL32

/* Performs multiplication of two complex_fract32s with 
** saturation.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  cmplx_mul32(long long  __a, long long  __b) {
  long long  __rval = __builtin_cmplx_mul32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CMPLX_MUL32 */

#if (!defined(__DEFINED_SHR_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHR_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHR_FR1X16))))

#define __DEFINED_SHR_FR1X16

/* Arithmetically shifts __a right by __b places
** with sign extension. If the __b amount is negative, the
** shift is to the left by abs(__b) places, and the empty bits
** are zero-filled.
** 
** WARNING: All the bits except for the lowest 5 bits of
** __b are ignored. If you want to shift by numbers larger
** than 5 bit signed ints, you should use shr_fr1x16_clip.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  shr_fr1x16(fract16  __a, short  __b) {
  fract16  __rval = __builtin_shr_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHR_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SHR_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHR_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHR_FX1X16))))

#define __DEFINED_SHR_FX1X16

/* Arithmetically shifts __a right by __b places
** with sign extension. If the __b amount is negative, the
** shift is to the left by abs(__b) places, and the empty bits
** are zero-filled.
** 
** WARNING: All the bits except for the lowest 5 bits of
** __b are ignored. If you want to shift by numbers larger
** than 5 bit signed ints, you should use shr_fx1x16_clip.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  shr_fx1x16(_Fract  __a, short  __b) {
  _Fract  __rval = __builtin_shr_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHR_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_SHR_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHR_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHR_FR1X32))))

#define __DEFINED_SHR_FR1X32

/* Arithmetically shifts the src variable right by shft places
** with sign extension. If the shift amount is negative, the
** shift is to the left by abs(shft) places, and the empty bits
** are zero-filled.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  shr_fr1x32(fract32  __a, short  __b) {
  fract32  __rval = __builtin_shr_fr1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHR_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SHR_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHR_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHR_FX1X32))))

#define __DEFINED_SHR_FX1X32

/* Arithmetically shifts the src variable right by shft places
** with sign extension. If the shift amount is negative, the
** shift is to the left by abs(shft) places, and the empty bits
** are zero-filled.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  shr_fx1x32(long _Fract  __a, short  __b) {
  long _Fract  __rval = __builtin_shr_fx1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHR_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_SHR_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHR_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHR_FR2X16))))

#define __DEFINED_SHR_FR2X16

/* Arithmetically shifts both fract16s in the fract2x16 right
** by __b places with sign extension, and returns the packed
** result. If __b is negative, the shift is to the left by
** abs(__b) places and the empty bits are zero-filled.
** 
** WARNING: All the bits except for the lowest 5 bits of
** __b are ignored. If you want to shift by numbers larger
** than 5 bit signed ints, you should use shr_fr2x16_clip.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  shr_fr2x16(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_shr_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHR_FR2X16 */

#if (!defined(__DEFINED_SHR_I1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHR_I1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHR_I1X16))))

#define __DEFINED_SHR_I1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  shr_i1x16(short  __a, short  __b) {
  short  __rval = __builtin_shr_i1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHR_I1X16 */

#if (!defined(__DEFINED_SHRL_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHRL_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHRL_FR2X16))))

#define __DEFINED_SHRL_FR2X16

/* Logically shifts both fract16s in the fract2x16 right by
** __b places. There is no sign extension and no saturation
** - the empty bits are zero-filled.
** 
** WARNING: All the bits except for the lowest 5 bits of
** __b are ignored. If you want to shift by numbers larger
** than 5 bit signed ints, you should use shrl_fr2x16_clip.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  shrl_fr2x16(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_shrl_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHRL_FR2X16 */

#if (!defined(__DEFINED_SHR_FR1X32_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHR_FR1X32_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHR_FR1X32_CLIP))))

#define __DEFINED_SHR_FR1X32_CLIP

/* As for shr_fr1x32 but also ensures __b is clipped to 6 bits
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  shr_fr1x32_clip(fract32  __a, short  __b) {
  fract32  __rval = __builtin_shr_fr1x32_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHR_FR1X32_CLIP */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SHR_FX1X32_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHR_FX1X32_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHR_FX1X32_CLIP))))

#define __DEFINED_SHR_FX1X32_CLIP

/* As for shr_fx1x32 but also ensures __b is clipped to 6 bits
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  shr_fx1x32_clip(long _Fract  __a, short  __b) {
  long _Fract  __rval = __builtin_shr_fx1x32_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHR_FX1X32_CLIP */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_SHR_FR2X16_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHR_FR2X16_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHR_FR2X16_CLIP))))

#define __DEFINED_SHR_FR2X16_CLIP

/* As shr_fr2x16 but also ensures __b is clipped to 5 bits
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  shr_fr2x16_clip(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_shr_fr2x16_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHR_FR2X16_CLIP */

#if (!defined(__DEFINED_SHRL_FR2X16_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHRL_FR2X16_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHRL_FR2X16_CLIP))))

#define __DEFINED_SHRL_FR2X16_CLIP

/* As shrl_fr2x16 but also ensures __b is clipped to 5 bits
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  shrl_fr2x16_clip(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_shrl_fr2x16_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHRL_FR2X16_CLIP */

#if (!defined(__DEFINED_SHL_FR1X32_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_FR1X32_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_FR1X32_CLIP))))

#define __DEFINED_SHL_FR1X32_CLIP

/* As for shl_fr1x32 but also ensures __b is clipped to 6 bits
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  shl_fr1x32_clip(fract32  __a, short  __b) {
  fract32  __rval = __builtin_shl_fr1x32_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_FR1X32_CLIP */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SHL_FX1X32_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_FX1X32_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_FX1X32_CLIP))))

#define __DEFINED_SHL_FX1X32_CLIP

/* As for shl_fx1x32 but also ensures __b is clipped to 6 bits
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  shl_fx1x32_clip(long _Fract  __a, short  __b) {
  long _Fract  __rval = __builtin_shl_fx1x32_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_FX1X32_CLIP */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_SHL_FR2X16_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_FR2X16_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_FR2X16_CLIP))))

#define __DEFINED_SHL_FR2X16_CLIP

/* As shl_fr2x16 but also ensures ___b is clipped to 5 bits
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  shl_fr2x16_clip(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_shl_fr2x16_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_FR2X16_CLIP */

#if (!defined(__DEFINED_MULT64_32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT64_32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT64_32X32))))

#define __DEFINED_MULT64_32X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  mult64_32x32(int  __a, int  __b) {
  long long  __rval = __builtin_mult64_32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT64_32X32 */

#if (!defined(__DEFINED_MULTU64_32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTU64_32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTU64_32X32))))

#define __DEFINED_MULTU64_32X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned long long  multu64_32x32(unsigned int  __a, unsigned int  __b) {
  unsigned long long  __rval = __builtin_multu64_32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTU64_32X32 */

#if (!defined(__DEFINED_ADDCLIP_LO) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADDCLIP_LO)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADDCLIP_LO))))

#define __DEFINED_ADDCLIP_LO

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  addclip_lo(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_addclip_lo(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_ADDCLIP_LO */

#if (!defined(__DEFINED_ADDCLIP_HI) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADDCLIP_HI)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADDCLIP_HI))))

#define __DEFINED_ADDCLIP_HI

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  addclip_hi(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_addclip_hi(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_ADDCLIP_HI */

#if (!defined(__DEFINED_ADDCLIP_LOR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADDCLIP_LOR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADDCLIP_LOR))))

#define __DEFINED_ADDCLIP_LOR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  addclip_lor(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_addclip_lor(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_ADDCLIP_LOR */

#if (!defined(__DEFINED_ADDCLIP_HIR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADDCLIP_HIR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADDCLIP_HIR))))

#define __DEFINED_ADDCLIP_HIR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  addclip_hir(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_addclip_hir(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_ADDCLIP_HIR */

#if (!defined(__DEFINED_EXTRACT_AND_ADD) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EXTRACT_AND_ADD)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EXTRACT_AND_ADD))))

#define __DEFINED_EXTRACT_AND_ADD

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  extract_and_add(int  __a, int  __b, int  *__r2) {
  int  __rval = __builtin_extract_and_add_r1(__a, __b);
  *__r2 = __builtin_extract_and_add_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_EXTRACT_AND_ADD */

#if (!defined(__DEFINED_ADD_I4X8) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_I4X8)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_I4X8))))

#define __DEFINED_ADD_I4X8

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  add_i4x8(long long  __a, char * __b, long long  __c, char * __d, int  *__r2) {
  int  __rval = __builtin_add_i4x8_r1(__a, __b, __c, __d);
  *__r2 = __builtin_add_i4x8_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_ADD_I4X8 */

#if (!defined(__DEFINED_ADD_I4X8_R) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_I4X8_R)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_I4X8_R))))

#define __DEFINED_ADD_I4X8_R

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  add_i4x8_r(long long  __a, char * __b, long long  __c, char * __d, int  *__r2) {
  int  __rval = __builtin_add_i4x8_r_r1(__a, __b, __c, __d);
  *__r2 = __builtin_add_i4x8_r_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_ADD_I4X8_R */

#if (!defined(__DEFINED_AVG_I4X8) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I4X8)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I4X8))))

#define __DEFINED_AVG_I4X8

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i4x8(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_avg_i4x8(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_AVG_I4X8 */

#if (!defined(__DEFINED_AVG_I4X8_T) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I4X8_T)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I4X8_T))))

#define __DEFINED_AVG_I4X8_T

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i4x8_t(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_avg_i4x8_t(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_AVG_I4X8_T */

#if (!defined(__DEFINED_AVG_I4X8_R) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I4X8_R)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I4X8_R))))

#define __DEFINED_AVG_I4X8_R

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i4x8_r(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_avg_i4x8_r(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_AVG_I4X8_R */

#if (!defined(__DEFINED_AVG_I4X8_TR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I4X8_TR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I4X8_TR))))

#define __DEFINED_AVG_I4X8_TR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i4x8_tr(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_avg_i4x8_tr(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_AVG_I4X8_TR */

#if (!defined(__DEFINED_AVG_I2X8_LO) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I2X8_LO)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I2X8_LO))))

#define __DEFINED_AVG_I2X8_LO

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i2x8_lo(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_avg_i2x8_lo(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_AVG_I2X8_LO */

#if (!defined(__DEFINED_AVG_I2X8_LOT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I2X8_LOT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I2X8_LOT))))

#define __DEFINED_AVG_I2X8_LOT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i2x8_lot(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_avg_i2x8_lot(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_AVG_I2X8_LOT */

#if (!defined(__DEFINED_AVG_I2X8_LOR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I2X8_LOR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I2X8_LOR))))

#define __DEFINED_AVG_I2X8_LOR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i2x8_lor(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_avg_i2x8_lor(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_AVG_I2X8_LOR */

#if (!defined(__DEFINED_AVG_I2X8_LOTR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I2X8_LOTR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I2X8_LOTR))))

#define __DEFINED_AVG_I2X8_LOTR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i2x8_lotr(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_avg_i2x8_lotr(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_AVG_I2X8_LOTR */

#if (!defined(__DEFINED_AVG_I2X8_HI) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I2X8_HI)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I2X8_HI))))

#define __DEFINED_AVG_I2X8_HI

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i2x8_hi(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_avg_i2x8_hi(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_AVG_I2X8_HI */

#if (!defined(__DEFINED_AVG_I2X8_HIT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I2X8_HIT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I2X8_HIT))))

#define __DEFINED_AVG_I2X8_HIT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i2x8_hit(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_avg_i2x8_hit(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_AVG_I2X8_HIT */

#if (!defined(__DEFINED_AVG_I2X8_HIR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I2X8_HIR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I2X8_HIR))))

#define __DEFINED_AVG_I2X8_HIR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i2x8_hir(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_avg_i2x8_hir(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_AVG_I2X8_HIR */

#if (!defined(__DEFINED_AVG_I2X8_HITR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_AVG_I2X8_HITR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_AVG_I2X8_HITR))))

#define __DEFINED_AVG_I2X8_HITR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  avg_i2x8_hitr(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_avg_i2x8_hitr(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_AVG_I2X8_HITR */

#if (!defined(__DEFINED_SUB_I4X8) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUB_I4X8)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUB_I4X8))))

#define __DEFINED_SUB_I4X8

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  sub_i4x8(long long  __a, char * __b, long long  __c, char * __d, int  *__r2) {
  int  __rval = __builtin_sub_i4x8_r1(__a, __b, __c, __d);
  *__r2 = __builtin_sub_i4x8_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_SUB_I4X8 */

#if (!defined(__DEFINED_SUB_I4X8_R) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUB_I4X8_R)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUB_I4X8_R))))

#define __DEFINED_SUB_I4X8_R

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  sub_i4x8_r(long long  __a, char * __b, long long  __c, char * __d, int  *__r2) {
  int  __rval = __builtin_sub_i4x8_r_r1(__a, __b, __c, __d);
  *__r2 = __builtin_sub_i4x8_r_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_SUB_I4X8_R */

#if (!defined(__DEFINED_SAA) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SAA)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SAA))))

#define __DEFINED_SAA

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  saa(long long  __a, char * __b, long long  __c, char * __d, int  __e, int  __f, int  *__r2) {
  int  __rval = __builtin_saa_r1(__a, __b, __c, __d, __e, __f);
  *__r2 = __builtin_saa_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_SAA */

#if (!defined(__DEFINED_SAA_R) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SAA_R)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SAA_R))))

#define __DEFINED_SAA_R

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  saa_r(long long  __a, char * __b, long long  __c, char * __d, int  __e, int  __f, int  *__r2) {
  int  __rval = __builtin_saa_r_r1(__a, __b, __c, __d, __e, __f);
  *__r2 = __builtin_saa_r_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_SAA_R */

#if (!defined(__DEFINED_BYTEUNPACK) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BYTEUNPACK)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BYTEUNPACK))))

#define __DEFINED_BYTEUNPACK

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  byteunpack(long long  __a, char * __b, int  *__r2) {
  int  __rval = __builtin_byteunpack_r1(__a, __b);
  *__r2 = __builtin_byteunpack_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BYTEUNPACK */

#if (!defined(__DEFINED_BYTEUNPACKR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BYTEUNPACKR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BYTEUNPACKR))))

#define __DEFINED_BYTEUNPACKR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  byteunpackr(long long  __a, char * __b, int  *__r2) {
  int  __rval = __builtin_byteunpackr_r1(__a, __b);
  *__r2 = __builtin_byteunpackr_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BYTEUNPACKR */

#if (!defined(__DEFINED_BITMUX_SHR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BITMUX_SHR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BITMUX_SHR))))

#define __DEFINED_BITMUX_SHR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  bitmux_shr(long long  __a, int  __b, int  __c, int  *__r2, int  *__r3) {
  long long  __rval = __builtin_bitmux_shr_r1(__a, __b, __c);
  *__r2 = __builtin_bitmux_shr_r2(__rval);
  *__r3 = __builtin_bitmux_shr_r3(__rval);
  return __rval;
}

#endif /* __DEFINED_BITMUX_SHR */

#if (!defined(__DEFINED_BITMUX_SHL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BITMUX_SHL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BITMUX_SHL))))

#define __DEFINED_BITMUX_SHL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  bitmux_shl(long long  __a, int  __b, int  __c, int  *__r2, int  *__r3) {
  long long  __rval = __builtin_bitmux_shl_r1(__a, __b, __c);
  *__r2 = __builtin_bitmux_shl_r2(__rval);
  *__r3 = __builtin_bitmux_shl_r3(__rval);
  return __rval;
}

#endif /* __DEFINED_BITMUX_SHL */

#if (!defined(__DEFINED_A_BXORSHIFT_MASK32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_BXORSHIFT_MASK32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_BXORSHIFT_MASK32))))

#define __DEFINED_A_BXORSHIFT_MASK32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  A_bxorshift_mask32(acc40  __a, int  __b, int  *__r2) {
  short  __rval = __builtin_A_bxorshift_mask32_r1(__a, __b);
  *__r2 = __builtin_A_bxorshift_mask32_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_A_BXORSHIFT_MASK32 */

#if (!defined(__DEFINED_A_BXOR_MASK32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_BXOR_MASK32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_BXOR_MASK32))))

#define __DEFINED_A_BXOR_MASK32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  A_bxor_mask32(acc40  __a, int  __b, int  *__r2) {
  short  __rval = __builtin_A_bxor_mask32_r1(__a, __b);
  *__r2 = __builtin_A_bxor_mask32_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_A_BXOR_MASK32 */

#if (!defined(__DEFINED_A_BXORSHIFT_MASK40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_BXORSHIFT_MASK40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_BXORSHIFT_MASK40))))

#define __DEFINED_A_BXORSHIFT_MASK40

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_bxorshift_mask40(acc40  __a, acc40  __b, int  __c) {
  acc40  __rval = __builtin_A_bxorshift_mask40(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_BXORSHIFT_MASK40 */

#if (!defined(__DEFINED_A_BXOR_MASK40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_BXOR_MASK40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_BXOR_MASK40))))

#define __DEFINED_A_BXOR_MASK40

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  A_bxor_mask40(acc40  __a, acc40  __b, int  __c, int  *__r2) {
  short  __rval = __builtin_A_bxor_mask40_r1(__a, __b, __c);
  *__r2 = __builtin_A_bxor_mask40_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_A_BXOR_MASK40 */

#if (!defined(__DEFINED_UNTESTSET) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_UNTESTSET)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_UNTESTSET))))

#define __DEFINED_UNTESTSET

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  untestset(char * __a) {
  __builtin_untestset(__a);
}

#endif /* __DEFINED_UNTESTSET */

#if (!defined(__DEFINED_SYSREG_READ64) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SYSREG_READ64)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SYSREG_READ64))))

#define __DEFINED_SYSREG_READ64

/* Read a 64-bit register value
*/

#define sysreg_read64(A) (__builtin_sysreg_read64((A)))

#endif /* __DEFINED_SYSREG_READ64 */

#if (!defined(__DEFINED_SYSREG_WRITE64) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SYSREG_WRITE64)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SYSREG_WRITE64))))

#define __DEFINED_SYSREG_WRITE64

/* Write a 64-bit register value
*/

#define sysreg_write64(A, B) (__builtin_sysreg_write64((A), (B)))

#endif /* __DEFINED_SYSREG_WRITE64 */

#if (!defined(__DEFINED_CSYNC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CSYNC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CSYNC))))

#define __DEFINED_CSYNC

/* generate a csync instruction protected by CLI/STI for anomaly 05000312;
** you can generate an unprotected csync by using csync_int
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  csync(void) {
  __builtin_csync();
}

#endif /* __DEFINED_CSYNC */

#if (!defined(__DEFINED_SSYNC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SSYNC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SSYNC))))

#define __DEFINED_SSYNC

/* generate a ssync instruction protected by CLI/STI for anomaly 05000312;
** you can generate an unprotected ssync by using ssync_int
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  ssync(void) {
  __builtin_ssync();
}

#endif /* __DEFINED_SSYNC */

#if (!defined(__DEFINED_SYSREG_READ) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SYSREG_READ)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SYSREG_READ))))

#define __DEFINED_SYSREG_READ

#define sysreg_read(A) (__builtin_sysreg_read((A)))

#endif /* __DEFINED_SYSREG_READ */

#if (!defined(__DEFINED_SYSREG_WRITE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SYSREG_WRITE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SYSREG_WRITE))))

#define __DEFINED_SYSREG_WRITE

#define sysreg_write(A, B) (__builtin_sysreg_write((A), (B)))

#endif /* __DEFINED_SYSREG_WRITE */

#if (!defined(__DEFINED_EMUCLK) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EMUCLK)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EMUCLK))))

#define __DEFINED_EMUCLK

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned long long  emuclk(void) {
  unsigned long long  __rval = __builtin_emuclk();
  return __rval;
}

#endif /* __DEFINED_EMUCLK */

#if (!defined(__DEFINED_A_SAT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_SAT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_SAT))))

#define __DEFINED_A_SAT

/* An = An (S)
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_sat(acc40  __a) {
  acc40  __rval = __builtin_A_sat(__a);
  return __rval;
}

#endif /* __DEFINED_A_SAT */

#if (!defined(__DEFINED_A_MAD) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MAD)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MAD))))

#define __DEFINED_A_MAD

/* Dn = Ax
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  A_mad(acc40  __a) {
  fract32  __rval = __builtin_A_mad(__a);
  return __rval;
}

#endif /* __DEFINED_A_MAD */

#if (!defined(__DEFINED_A_MAD_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MAD_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MAD_FU))))

#define __DEFINED_A_MAD_FU

/* Dn = Ax (FU)
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  A_mad_FU(acc40  __a) {
  fract32  __rval = __builtin_A_mad_FU(__a);
  return __rval;
}

#endif /* __DEFINED_A_MAD_FU */

#if (!defined(__DEFINED_A_MAD_S2RND) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MAD_S2RND)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MAD_S2RND))))

#define __DEFINED_A_MAD_S2RND

/* Dn = Ax (S2RND)
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  A_mad_S2RND(acc40  __a) {
  fract32  __rval = __builtin_A_mad_S2RND(__a);
  return __rval;
}

#endif /* __DEFINED_A_MAD_S2RND */

#if (!defined(__DEFINED_A_MAD_ISS2) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MAD_ISS2)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MAD_ISS2))))

#define __DEFINED_A_MAD_ISS2

/* Dn = Ax (ISS2)
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  A_mad_ISS2(acc40  __a) {
  int  __rval = __builtin_A_mad_ISS2(__a);
  return __rval;
}

#endif /* __DEFINED_A_MAD_ISS2 */

#if (!defined(__DEFINED_A_MADH_IS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MADH_IS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MADH_IS))))

#define __DEFINED_A_MADH_IS

/* Dn.lh = Ax (IS)
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  A_madh_IS(acc40  __a) {
  short  __rval = __builtin_A_madh_IS(__a);
  return __rval;
}

#endif /* __DEFINED_A_MADH_IS */

#if (!defined(__DEFINED_A_MADH_IU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MADH_IU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MADH_IU))))

#define __DEFINED_A_MADH_IU

/* Dn.lh = Ax (IU)
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned short  A_madh_IU(acc40  __a) {
  unsigned short  __rval = __builtin_A_madh_IU(__a);
  return __rval;
}

#endif /* __DEFINED_A_MADH_IU */

#if (!defined(__DEFINED_A_MADH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MADH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MADH))))

#define __DEFINED_A_MADH

/* Dn.lh = Ax
** Whether the rounding is biased or unbiased depends on what
** the RND_MOD bit in the ASTAT register is set to.
** 
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  A_madh(acc40  __a) {
  fract16  __rval = __builtin_A_madh(__a);
  return __rval;
}

#endif /* __DEFINED_A_MADH */

#if (!defined(__DEFINED_A_MADH_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MADH_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MADH_FU))))

#define __DEFINED_A_MADH_FU

/* Dn.lh = Ax (FU)
** Whether the rounding is biased or unbiased depends on what
** the RND_MOD bit in the ASTAT register is set to.
** 
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  A_madh_FU(acc40  __a) {
  fract16  __rval = __builtin_A_madh_FU(__a);
  return __rval;
}

#endif /* __DEFINED_A_MADH_FU */

#if (!defined(__DEFINED_A_MADH_T) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MADH_T)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MADH_T))))

#define __DEFINED_A_MADH_T

/* Dn.lh = Ax (T)
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  A_madh_T(acc40  __a) {
  fract16  __rval = __builtin_A_madh_T(__a);
  return __rval;
}

#endif /* __DEFINED_A_MADH_T */

#if (!defined(__DEFINED_A_MADH_TFU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MADH_TFU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MADH_TFU))))

#define __DEFINED_A_MADH_TFU

/* Dn.lh = Ax (TFU)
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  A_madh_TFU(acc40  __a) {
  fract16  __rval = __builtin_A_madh_TFU(__a);
  return __rval;
}

#endif /* __DEFINED_A_MADH_TFU */

#if (!defined(__DEFINED_A_MADH_S2RND) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MADH_S2RND)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MADH_S2RND))))

#define __DEFINED_A_MADH_S2RND

/* Dn.lh = Ax (S2RND)
** Whether the rounding is biased or unbiased depends on what
** the RND_MOD bit in the ASTAT register is set to.
** 
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  A_madh_S2RND(acc40  __a) {
  fract16  __rval = __builtin_A_madh_S2RND(__a);
  return __rval;
}

#endif /* __DEFINED_A_MADH_S2RND */

#if (!defined(__DEFINED_A_MADH_ISS2) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MADH_ISS2)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MADH_ISS2))))

#define __DEFINED_A_MADH_ISS2

/* Dn.lh = Ax (ISS2)
** Whether the rounding is biased or unbiased depends on what
** the RND_MOD bit in the ASTAT register is set to.
** 
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  A_madh_ISS2(acc40  __a) {
  short  __rval = __builtin_A_madh_ISS2(__a);
  return __rval;
}

#endif /* __DEFINED_A_MADH_ISS2 */

#if (!defined(__DEFINED_A_MADH_IH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MADH_IH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MADH_IH))))

#define __DEFINED_A_MADH_IH

/* Dn.lh = Ax (IH)
** Whether the rounding is biased or unbiased depends on what
** the RND_MOD bit in the ASTAT register is set to.
** 
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  A_madh_IH(acc40  __a) {
  short  __rval = __builtin_A_madh_IH(__a);
  return __rval;
}

#endif /* __DEFINED_A_MADH_IH */

#if (!defined(__DEFINED_A_EQ) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_EQ)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_EQ))))

#define __DEFINED_A_EQ

/* CC = A0 == A1
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  A_eq(acc40  __a, acc40  __b) {
  int  __rval = __builtin_A_eq(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_EQ */

#if (!defined(__DEFINED_A_LT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_LT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_LT))))

#define __DEFINED_A_LT

/* CC = A0 < A1
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  A_lt(acc40  __a, acc40  __b) {
  int  __rval = __builtin_A_lt(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_LT */

#if (!defined(__DEFINED_A_LE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_LE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_LE))))

#define __DEFINED_A_LE

/* CC = A0 <= A1
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  A_le(acc40  __a, acc40  __b) {
  int  __rval = __builtin_A_le(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_LE */

#if (!defined(__DEFINED_ONES) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ONES)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ONES))))

#define __DEFINED_ONES

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  ones(int  __a) {
  short  __rval = __builtin_ones(__a);
  return __rval;
}

#endif /* __DEFINED_ONES */

#if (!defined(__DEFINED_A_BITMUX_ASL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_BITMUX_ASL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_BITMUX_ASL))))

#define __DEFINED_A_BITMUX_ASL

/* BITMUX(Dx, Dy, A0) (ASL)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  A_bitmux_ASL(int  __a, int  __b, acc40  __c, int  *__r2, acc40  *__r3) {
  int  __rval = __builtin_A_bitmux_ASL_r1(__a, __b, __c);
  *__r2 = __builtin_A_bitmux_ASL_r2(__rval);
  *__r3 = __builtin_A_bitmux_ASL_r3(__rval);
  return __rval;
}

#endif /* __DEFINED_A_BITMUX_ASL */

#if (!defined(__DEFINED_A_BITMUX_ASR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_BITMUX_ASR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_BITMUX_ASR))))

#define __DEFINED_A_BITMUX_ASR

/* BITMUX(Dx, Dy, A0) (ASR)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  A_bitmux_ASR(int  __a, int  __b, acc40  __c, int  *__r2, acc40  *__r3) {
  int  __rval = __builtin_A_bitmux_ASR_r1(__a, __b, __c);
  *__r2 = __builtin_A_bitmux_ASR_r2(__rval);
  *__r3 = __builtin_A_bitmux_ASR_r3(__rval);
  return __rval;
}

#endif /* __DEFINED_A_BITMUX_ASR */

#if (!defined(__DEFINED_SHL_I1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_I1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_I1X16))))

#define __DEFINED_SHL_I1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  shl_i1x16(short  __a, short  __b) {
  short  __rval = __builtin_shl_i1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_I1X16 */

#if (!defined(__DEFINED_SHL_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_I2X16))))

#define __DEFINED_SHL_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  shl_i2x16(int  __a, short  __b) {
  int  __rval = __builtin_shl_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_I2X16 */

#if (!defined(__DEFINED_SHL_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_FR1X16))))

#define __DEFINED_SHL_FR1X16

/* Arithmetically shifts the __a variable left by __b places.
** The empty bits are zero filled. If __b is negative, the shift
** is to the right by abs(__b) places with sign extension.
** 
** WARNING: All the bits except for the lowest 5 bits
** of __b are ignored. To shift by numbers larger than 5 bit
** signed ints, shl_fr1x16_clip should be used.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  shl_fr1x16(fract16  __a, short  __b) {
  fract16  __rval = __builtin_shl_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SHL_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_FX1X16))))

#define __DEFINED_SHL_FX1X16

/* Arithmetically shifts the __a variable left by __b places.
** The empty bits are zero filled. If __b is negative, the shift
** is to the right by abs(__b) places with sign extension.
** 
** WARNING: All the bits except for the lowest 5 bits
** of __b are ignored. To shift by numbers larger than 5 bit
** signed ints, shl_fx1x16_clip should be used.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  shl_fx1x16(_Fract  __a, short  __b) {
  _Fract  __rval = __builtin_shl_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_SHL_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_FR2X16))))

#define __DEFINED_SHL_FR2X16

/* Arithmetically shifts both fract16s in the fract2x16 left by __b
** places, and returns the packed result. The empty bits are zero
** filled. If __b is negative, the shift is to the right by
** abs(__b) places with sign extension.
** 
** WARNING: All the bits except for the lowest 5 bits of
** __b are ignored. If you want to shift by numbers larger
** than 5 bit signed ints, you should use shl_fr2x16_clip.
** 
** Inputs: __a{a,b} __b
** Returns: {a<<__b,b<<__b}
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  shl_fr2x16(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_shl_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_FR2X16 */

#if (!defined(__DEFINED_SHL_I1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_I1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_I1X32))))

#define __DEFINED_SHL_I1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  shl_i1x32(int  __a, short  __b) {
  int  __rval = __builtin_shl_i1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_I1X32 */

#if (!defined(__DEFINED_SHL_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_FR1X32))))

#define __DEFINED_SHL_FR1X32

/* Arithmetically shifts __a left by __b places.
** The empty bits are zero filled. If __b is negative,
** the shift is to the right by abs(__b) places with
** sign extension.
** 
** WARNING: All the bits except for the lowest 6 bits of
** __b are ignored. If you want to shift by numbers larger
** than 6 bit signed ints, you should use shl_fr1x32_clip.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  shl_fr1x32(fract32  __a, short  __b) {
  fract32  __rval = __builtin_shl_fr1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SHL_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHL_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHL_FX1X32))))

#define __DEFINED_SHL_FX1X32

/* Arithmetically shifts __a left by __b places.
** The empty bits are zero filled. If __b is negative,
** the shift is to the right by abs(__b) places with
** sign extension.
** 
** WARNING: All the bits except for the lowest 6 bits of
** __b are ignored. If you want to shift by numbers larger
** than 6 bit signed ints, you should use shl_fx1x32_clip.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  shl_fx1x32(long _Fract  __a, short  __b) {
  long _Fract  __rval = __builtin_shl_fx1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHL_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_A_ASHIFT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_ASHIFT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_ASHIFT))))

#define __DEFINED_A_ASHIFT

/* An = ASHIFT An BY Dx.L   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_ashift(acc40  __a, short  __b) {
  acc40  __rval = __builtin_A_ashift(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_ASHIFT */

#if (!defined(__DEFINED_SHLL_I1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHLL_I1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHLL_I1X16))))

#define __DEFINED_SHLL_I1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  shll_i1x16(short  __a, short  __b) {
  short  __rval = __builtin_shll_i1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHLL_I1X16 */

#if (!defined(__DEFINED_SHLL_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHLL_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHLL_I2X16))))

#define __DEFINED_SHLL_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  shll_i2x16(int  __a, short  __b) {
  int  __rval = __builtin_shll_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHLL_I2X16 */

#if (!defined(__DEFINED_SHLL_I1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SHLL_I1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SHLL_I1X32))))

#define __DEFINED_SHLL_I1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  shll_i1x32(int  __a, short  __b) {
  int  __rval = __builtin_shll_i1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SHLL_I1X32 */

#if (!defined(__DEFINED_A_LSHIFT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_LSHIFT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_LSHIFT))))

#define __DEFINED_A_LSHIFT

/* An = LSHIFT An BY Dx.L   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_lshift(acc40  __a, short  __b) {
  acc40  __rval = __builtin_A_lshift(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_LSHIFT */

#if (!defined(__DEFINED_ABS_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ABS_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ABS_FR1X32))))

#define __DEFINED_ABS_FR1X32

/* Returns the 32-bit value that is the absolute value of the
** input parameter. Where the input is 0x80000000, saturation
** occurs and 0x7fffffff is returned.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  abs_fr1x32(fract32  __a) {
  fract32  __rval = __builtin_abs_fr1x32(__a);
  return __rval;
}

#endif /* __DEFINED_ABS_FR1X32 */

#if (!defined(__DEFINED_ABS_I1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ABS_I1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ABS_I1X32))))

#define __DEFINED_ABS_I1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  abs_i1x32(int  __a) {
  int  __rval = __builtin_abs_i1x32(__a);
  return __rval;
}

#endif /* __DEFINED_ABS_I1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_ABS_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ABS_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ABS_FX1X32))))

#define __DEFINED_ABS_FX1X32

/* Returns the 32-bit value that is the absolute value of the
** input parameter. Where the input is 0x80000000, saturation
** occurs and 0x7fffffff is returned.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  abs_fx1x32(long _Fract  __a) {
  long _Fract  __rval = __builtin_abs_fx1x32(__a);
  return __rval;
}

#endif /* __DEFINED_ABS_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_A_ABS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_ABS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_ABS))))

#define __DEFINED_A_ABS

/* An = ABS An   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_abs(acc40  __a) {
  acc40  __rval = __builtin_A_abs(__a);
  return __rval;
}

#endif /* __DEFINED_A_ABS */

#if (!defined(__DEFINED_ABS_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ABS_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ABS_FR2X16))))

#define __DEFINED_ABS_FR2X16

/* Returns the absolute value of both fract16s in the fract2x16.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  abs_fr2x16(fract2x16  __a) {
  fract2x16  __rval = __builtin_abs_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_ABS_FR2X16 */

#if (!defined(__DEFINED_ABS_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ABS_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ABS_I2X16))))

#define __DEFINED_ABS_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  abs_i2x16(int  __a) {
  int  __rval = __builtin_abs_i2x16(__a);
  return __rval;
}

#endif /* __DEFINED_ABS_I2X16 */

#if (!defined(__DEFINED_A_SUB) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_SUB)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_SUB))))

#define __DEFINED_A_SUB

/* A1 -= A0   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_sub(acc40  __a, acc40  __b) {
  acc40  __rval = __builtin_A_sub(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_SUB */

#if (!defined(__DEFINED_A_ADD) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_ADD)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_ADD))))

#define __DEFINED_A_ADD

/* A0 += A1   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_add(acc40  __a, acc40  __b) {
  acc40  __rval = __builtin_A_add(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_ADD */

#if (!defined(__DEFINED_BREVADD) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BREVADD)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BREVADD))))

#define __DEFINED_BREVADD

/* Performs a bit-carry-reversed addition of the two pointer inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * brevadd(void * __a, void * __b) {
  void * __rval = __builtin_brevadd(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BREVADD */

#if (!defined(__DEFINED_DIVS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_DIVS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_DIVS))))

#define __DEFINED_DIVS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  divs(int  __a, int  __b, int  *__r2) {
  int  __rval = __builtin_divs_r1(__a, __b);
  *__r2 = __builtin_divs_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_DIVS */

#if (!defined(__DEFINED_DIVQ) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_DIVQ)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_DIVQ))))

#define __DEFINED_DIVQ

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  divq(int  __a, int  __b, int  __c, int  *__r2) {
  int  __rval = __builtin_divq_r1(__a, __b, __c);
  *__r2 = __builtin_divq_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_DIVQ */

#if (!defined(__DEFINED_EXPADJ1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EXPADJ1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EXPADJ1X32))))

#define __DEFINED_EXPADJ1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  expadj1x32(int  __a, short  __b) {
  short  __rval = __builtin_expadj1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_EXPADJ1X32 */

#if (!defined(__DEFINED_EXPADJ1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EXPADJ1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EXPADJ1X16))))

#define __DEFINED_EXPADJ1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  expadj1x16(short  __a, short  __b) {
  short  __rval = __builtin_expadj1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_EXPADJ1X16 */

#if (!defined(__DEFINED_EXPADJ2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EXPADJ2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EXPADJ2X16))))

#define __DEFINED_EXPADJ2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  expadj2x16(int  __a, short  __b) {
  short  __rval = __builtin_expadj2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_EXPADJ2X16 */

#if (!defined(__DEFINED_LVITMAX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_LVITMAX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_LVITMAX1X16))))

#define __DEFINED_LVITMAX1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  lvitmax1x16(int  __a, int  __b, int  *__r2) {
  short  __rval = __builtin_lvitmax1x16_r1(__a, __b);
  *__r2 = __builtin_lvitmax1x16_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_LVITMAX1X16 */

#if (!defined(__DEFINED_RVITMAX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_RVITMAX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_RVITMAX1X16))))

#define __DEFINED_RVITMAX1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  rvitmax1x16(int  __a, int  __b, int  *__r2) {
  short  __rval = __builtin_rvitmax1x16_r1(__a, __b);
  *__r2 = __builtin_rvitmax1x16_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_RVITMAX1X16 */

#if (!defined(__DEFINED_LVITMAX2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_LVITMAX2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_LVITMAX2X16))))

#define __DEFINED_LVITMAX2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  lvitmax2x16(int  __a, int  __b, int  __c, int  *__r2) {
  int  __rval = __builtin_lvitmax2x16_r1(__a, __b, __c);
  *__r2 = __builtin_lvitmax2x16_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_LVITMAX2X16 */

#if (!defined(__DEFINED_RVITMAX2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_RVITMAX2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_RVITMAX2X16))))

#define __DEFINED_RVITMAX2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  rvitmax2x16(int  __a, int  __b, int  __c, int  *__r2) {
  int  __rval = __builtin_rvitmax2x16_r1(__a, __b, __c);
  *__r2 = __builtin_rvitmax2x16_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_RVITMAX2X16 */

#if (!defined(__DEFINED_MAX_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MAX_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MAX_I2X16))))

#define __DEFINED_MAX_I2X16

/* Returns the maximums of the two pairs of shorts in the two
** packed inputs.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  max_i2x16(int  __a, int  __b) {
  int  __rval = __builtin_max_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MAX_I2X16 */

#if (!defined(__DEFINED_MAX_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MAX_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MAX_FR2X16))))

#define __DEFINED_MAX_FR2X16

/* Returns the maximums of the two pairs of fract16s in the two
** input fract2x16s.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  max_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_max_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MAX_FR2X16 */

#if (!defined(__DEFINED_MIN_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MIN_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MIN_FR2X16))))

#define __DEFINED_MIN_FR2X16

/* Returns the minimums of the two pairs of fract16s in the two
** input fract2x16s.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  min_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_min_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MIN_FR2X16 */

#if (!defined(__DEFINED_MIN_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MIN_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MIN_I2X16))))

#define __DEFINED_MIN_I2X16

/* Returns the minimums of the two pairs of shorts in the two
** packed inputs.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  min_i2x16(int  __a, int  __b) {
  int  __rval = __builtin_min_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MIN_I2X16 */

#if (!defined(__DEFINED_NEGATE_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_NEGATE_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_NEGATE_FR1X32))))

#define __DEFINED_NEGATE_FR1X32

/* Returns the 32-bit result of the negation of the input
** parameter (-_x). If the input is 0x80000000, saturation
** occurs and 0x7fffffff is returned.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  negate_fr1x32(fract32  __a) {
  fract32  __rval = __builtin_negate_fr1x32(__a);
  return __rval;
}

#endif /* __DEFINED_NEGATE_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_NEGATE_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_NEGATE_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_NEGATE_FX1X32))))

#define __DEFINED_NEGATE_FX1X32

/* Returns the 32-bit result of the negation of the input
** parameter (-_x). If the input is 0x80000000, saturation
** occurs and 0x7fffffff is returned.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  negate_fx1x32(long _Fract  __a) {
  long _Fract  __rval = __builtin_negate_fx1x32(__a);
  return __rval;
}

#endif /* __DEFINED_NEGATE_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_A_NEG) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_NEG)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_NEG))))

#define __DEFINED_A_NEG

/* An = -An   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_neg(acc40  __a) {
  acc40  __rval = __builtin_A_neg(__a);
  return __rval;
}

#endif /* __DEFINED_A_NEG */

#if (!defined(__DEFINED_NEGATE_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_NEGATE_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_NEGATE_FR2X16))))

#define __DEFINED_NEGATE_FR2X16

/* Negates both 16 bit fracts in the packed fract. If one of the
** fract16 values is 0x8000, saturation occurs and 0x7fff is the
** result of the negation.
** Input: __a{a,b}
** Returns: {-a,-b}
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  negate_fr2x16(fract2x16  __a) {
  fract2x16  __rval = __builtin_negate_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_NEGATE_FR2X16 */

#if (!defined(__DEFINED_ROUND_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ROUND_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ROUND_FR1X32))))

#define __DEFINED_ROUND_FR1X32

/* This function rounds the 32-bit fract input to a 16-bit fract
** using biased rounding.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  round_fr1x32(fract32  __a) {
  fract16  __rval = __builtin_round_fr1x32(__a);
  return __rval;
}

#endif /* __DEFINED_ROUND_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_ROUND_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ROUND_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ROUND_FX1X32))))

#define __DEFINED_ROUND_FX1X32

/* This function rounds the 32-bit fract input to a 16-bit fract
** using biased rounding.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  round_fx1x32(long _Fract  __a) {
  _Fract  __rval = __builtin_round_fx1x32(__a);
  return __rval;
}

#endif /* __DEFINED_ROUND_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_NORM_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_NORM_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_NORM_FR1X32))))

#define __DEFINED_NORM_FR1X32

/* Returns the number of left shifts required to normalize the input
** variable so that it is either in the interval 0x40000000 to 0x7fffffff,
** or in the interval 0x80000000 to 0xc0000000. In other words,
** 
**  fract32 x;
**  shl_fr1x32(x,norm_fr1x32(x));
** 
** will return a value in the range 0x40000000 to 0x7fffffff, or in
** the range 0x80000000 to 0xc0000000.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  norm_fr1x32(fract32  __a) {
  short  __rval = __builtin_norm_fr1x32(__a);
  return __rval;
}

#endif /* __DEFINED_NORM_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_NORM_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_NORM_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_NORM_FX1X32))))

#define __DEFINED_NORM_FX1X32

/* Returns the number of left shifts required to normalize the input
** variable so that it is either in the interval 0x40000000 to 0x7fffffff,
** or in the interval 0x80000000 to 0xc0000000. In other words,
** 
**  fract x;
**  shl_fx1x32(x,norm_fx1x32(x));
** 
** will return a value in the range 0.5 to 1.0, or in the range 
** -1.0 to -0.5.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  norm_fx1x32(long _Fract  __a) {
  short  __rval = __builtin_norm_fx1x32(__a);
  return __rval;
}

#endif /* __DEFINED_NORM_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_NORM_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_NORM_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_NORM_FR1X16))))

#define __DEFINED_NORM_FR1X16

/* Returns the number of left shifts required to normalize the input
** variable so that it is either in the interval 0x4000 to 0x7fff,
** or in the interval 0x8000 to 0xc000. In other words,
** 
**  fract16 x;
**  shl_fr1x16(x,norm_fr1x16(x));
** 
** will return a value in the range 0x4000 to 0x7fff, or in the 
** range 0x8000 to 0xc000.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  norm_fr1x16(fract16  __a) {
  short  __rval = __builtin_norm_fr1x16(__a);
  return __rval;
}

#endif /* __DEFINED_NORM_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_NORM_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_NORM_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_NORM_FX1X16))))

#define __DEFINED_NORM_FX1X16

/* Returns the number of left shifts required to normalize the input
** variable so that it is either in the interval 0x4000 to 0x7fff,
** or in the interval 0x8000 to 0xc000. In other words,
** 
**  fract x;
**  shl_fx1x16(x,norm_fx1x16(x));
** 
** will return a value in the range 0.5 to 1.0, or in the 
** range -1.0 to -0.5.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  norm_fx1x16(_Fract  __a) {
  short  __rval = __builtin_norm_fx1x16(__a);
  return __rval;
}

#endif /* __DEFINED_NORM_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_A_SIGNBITS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_SIGNBITS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_SIGNBITS))))

#define __DEFINED_A_SIGNBITS

/* Dn.L = SIGNBITS Ax   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  A_signbits(acc40  __a) {
  short  __rval = __builtin_A_signbits(__a);
  return __rval;
}

#endif /* __DEFINED_A_SIGNBITS */

#if (!defined(__DEFINED_MULTR_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTR_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTR_FR1X16))))

#define __DEFINED_MULTR_FR1X16

/* Performs a 16-bit fractional multiplication of the two input
** parameters. The result is rounded to 16 bits. Whether the
** rounding is biased or unbiased depends what the RND_MOD bit
** in the ASTAT register is set to.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  multr_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_multr_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTR_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MULTR_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTR_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTR_FX1X16))))

#define __DEFINED_MULTR_FX1X16

/* Performs a 16-bit fractional multiplication of the two input
** parameters. The result is rounded to 16 bits. Whether the
** rounding is biased or unbiased depends what the RND_MOD bit
** in the ASTAT register is set to.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  multr_fx1x16(_Fract  __a, _Fract  __b) {
  _Fract  __rval = __builtin_multr_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTR_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MULTMR_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTMR_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTMR_FR1X16))))

#define __DEFINED_MULTMR_FR1X16

/* 16-bit signed by unsigned fractional multiplication.
** The result is rounded to 16 bits. Whether the
** rounding is biased or unbiased depends what the RND_MOD bit
** in the ASTAT register is set to.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  multmr_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_multmr_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTMR_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MULTMR_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTMR_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTMR_FX1X16))))

#define __DEFINED_MULTMR_FX1X16

/* 16-bit signed by unsigned fractional multiplication.
** The result is rounded to 16 bits. Whether the
** rounding is biased or unbiased depends what the RND_MOD bit
** in the ASTAT register is set to.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  multmr_fx1x16(_Fract  __a, unsigned _Fract  __b) {
  _Fract  __rval = __builtin_multmr_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTMR_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MULTM_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTM_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTM_FR1X16))))

#define __DEFINED_MULTM_FR1X16

/* 16-bit signed by unsigned fractional multiplication with the T option to generate a 16-bit truncated result
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  multm_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_multm_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTM_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MULTM_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTM_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTM_FX1X16))))

#define __DEFINED_MULTM_FX1X16

/* 16-bit signed by unsigned fractional multiplication with the T option to generate a 16-bit truncated result
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  multm_fx1x16(_Fract  __a, unsigned _Fract  __b) {
  _Fract  __rval = __builtin_multm_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTM_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_CMPLX_MUL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_MUL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_MUL))))

#define __DEFINED_CMPLX_MUL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  cmplx_mul(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_cmplx_mul(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CMPLX_MUL */

#if (!defined(__DEFINED_CMPLX_CONJ_MUL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_CONJ_MUL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_CONJ_MUL))))

#define __DEFINED_CMPLX_CONJ_MUL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  cmplx_conj_mul(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_cmplx_conj_mul(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CMPLX_CONJ_MUL */

#if (!defined(__DEFINED_CSQUMAG_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CSQUMAG_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CSQUMAG_FR16))))

#define __DEFINED_CSQUMAG_FR16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  csqumag_fr16(int  __a) {
  short  __rval = __builtin_csqumag_fr16(__a);
  return __rval;
}

#endif /* __DEFINED_CSQUMAG_FR16 */

#if (!defined(__DEFINED_CSQUMAG_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CSQUMAG_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CSQUMAG_FR32))))

#define __DEFINED_CSQUMAG_FR32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  csqumag_fr32(int  __a) {
  int  __rval = __builtin_csqumag_fr32(__a);
  return __rval;
}

#endif /* __DEFINED_CSQUMAG_FR32 */

#if (!defined(__DEFINED_A_MULT_IS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MULT_IS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MULT_IS))))

#define __DEFINED_A_MULT_IS

/* An = Rx.lh * Ry.lh (IS)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mult_IS(short  __a, short  __b) {
  acc40  __rval = __builtin_A_mult_IS(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_MULT_IS */

#if (!defined(__DEFINED_A_MAC_IS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MAC_IS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MAC_IS))))

#define __DEFINED_A_MAC_IS

/* An += Rx.lh * Ry.lh (IS)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mac_IS(acc40  __a, short  __b, short  __c) {
  acc40  __rval = __builtin_A_mac_IS(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MAC_IS */

#if (!defined(__DEFINED_A_MSU_IS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MSU_IS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MSU_IS))))

#define __DEFINED_A_MSU_IS

/* An -= Rx.lh * Ry.lh (IS)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_msu_IS(acc40  __a, short  __b, short  __c) {
  acc40  __rval = __builtin_A_msu_IS(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MSU_IS */

#if (!defined(__DEFINED_A_MULT_MIS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MULT_MIS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MULT_MIS))))

#define __DEFINED_A_MULT_MIS

/* A1 = Rx.lh * Ry.lh (M,IS)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mult_MIS(short  __a, unsigned short  __b) {
  acc40  __rval = __builtin_A_mult_MIS(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_MULT_MIS */

#if (!defined(__DEFINED_A_MAC_MIS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MAC_MIS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MAC_MIS))))

#define __DEFINED_A_MAC_MIS

/* A1 += Rx.lh * Ry.lh (M,IS)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mac_MIS(acc40  __a, short  __b, unsigned short  __c) {
  acc40  __rval = __builtin_A_mac_MIS(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MAC_MIS */

#if (!defined(__DEFINED_A_MSU_MIS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MSU_MIS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MSU_MIS))))

#define __DEFINED_A_MSU_MIS

/* A1 -= Rx.lh * Ry.lh (M,IS)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_msu_MIS(acc40  __a, short  __b, unsigned short  __c) {
  acc40  __rval = __builtin_A_msu_MIS(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MSU_MIS */

#if (!defined(__DEFINED_A_MULT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MULT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MULT))))

#define __DEFINED_A_MULT

/* An = Rx.lh * Ry.lh   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mult(fract16  __a, fract16  __b) {
  acc40  __rval = __builtin_A_mult(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_MULT */

#if (!defined(__DEFINED_A_MAC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MAC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MAC))))

#define __DEFINED_A_MAC

/* An += Rx.lh * Ry.lh   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mac(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_A_mac(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MAC */

#if (!defined(__DEFINED_A_MSU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MSU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MSU))))

#define __DEFINED_A_MSU

/* An -= Rx.lh * Ry.lh   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_msu(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_A_msu(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MSU */

#if (!defined(__DEFINED_A_MULT_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MULT_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MULT_FU))))

#define __DEFINED_A_MULT_FU

/* An = Rx.lh * Ry.lh (FU)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mult_FU(fract16  __a, fract16  __b) {
  acc40  __rval = __builtin_A_mult_FU(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_MULT_FU */

#if (!defined(__DEFINED_A_MAC_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MAC_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MAC_FU))))

#define __DEFINED_A_MAC_FU

/* An = Rx.lh * Ry.lh (FU)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mac_FU(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_A_mac_FU(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MAC_FU */

#if (!defined(__DEFINED_A_MSU_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MSU_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MSU_FU))))

#define __DEFINED_A_MSU_FU

/* An = Rx.lh * Ry.lh (FU)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_msu_FU(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_A_msu_FU(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MSU_FU */

#if (!defined(__DEFINED_A_MULT_M) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MULT_M)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MULT_M))))

#define __DEFINED_A_MULT_M

/* A1 = Rx.lh * Ry.lh (M)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mult_M(fract16  __a, fract16  __b) {
  acc40  __rval = __builtin_A_mult_M(__a, __b);
  return __rval;
}

#endif /* __DEFINED_A_MULT_M */

#if (!defined(__DEFINED_A_MAC_M) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MAC_M)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MAC_M))))

#define __DEFINED_A_MAC_M

/* A1 += Rx.lh * Ry.lh (M)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_mac_M(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_A_mac_M(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MAC_M */

#if (!defined(__DEFINED_A_MSU_M) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_A_MSU_M)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_A_MSU_M))))

#define __DEFINED_A_MSU_M

/* A1 -= Rx.lh * Ry.lh (M)   preserving the whole accumulator
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  A_msu_M(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_A_msu_M(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_A_MSU_M */

#if (!defined(__DEFINED_ADD_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_I2X16))))

#define __DEFINED_ADD_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  add_i2x16(int  __a, int  __b) {
  int  __rval = __builtin_add_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_I2X16 */

#if (!defined(__DEFINED_ADD_U2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_U2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_U2X16))))

#define __DEFINED_ADD_U2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned int  add_u2x16(unsigned int  __a, unsigned int  __b) {
  unsigned int  __rval = __builtin_add_u2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_U2X16 */

#if (!defined(__DEFINED_ADD_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_FR2X16))))

#define __DEFINED_ADD_FR2X16

/* Adds two packed fracts.
**   Input: _x{a,b} _y{c,d}
**   Returns: {a+c,b+d}
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  add_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_add_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_FR2X16 */

#if (!defined(__DEFINED_CMPLX_ADD) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_ADD)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_ADD))))

#define __DEFINED_CMPLX_ADD

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_add(int  __a, int  __b) {
  int  __rval = __builtin_cmplx_add(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CMPLX_ADD */

#if (!defined(__DEFINED_SUB_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUB_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUB_I2X16))))

#define __DEFINED_SUB_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  sub_i2x16(int  __a, int  __b) {
  int  __rval = __builtin_sub_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SUB_I2X16 */

#if (!defined(__DEFINED_SUB_U2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUB_U2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUB_U2X16))))

#define __DEFINED_SUB_U2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned int  sub_u2x16(unsigned int  __a, unsigned int  __b) {
  unsigned int  __rval = __builtin_sub_u2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SUB_U2X16 */

#if (!defined(__DEFINED_SUB_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUB_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUB_FR2X16))))

#define __DEFINED_SUB_FR2X16

/* Subtracts two packed fracts.
**   Input: __a{a,b} __b{c,d}
**   Returns: {a+c,b+d}
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  sub_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_sub_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SUB_FR2X16 */

#if (!defined(__DEFINED_CMPLX_SUB) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CMPLX_SUB)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CMPLX_SUB))))

#define __DEFINED_CMPLX_SUB

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  cmplx_sub(int  __a, int  __b) {
  int  __rval = __builtin_cmplx_sub(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CMPLX_SUB */

#if (!defined(__DEFINED_ADD_AS_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_AS_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_AS_FR2X16))))

#define __DEFINED_ADD_AS_FR2X16

/* Performs a vector add/subtract on the two input fract2x16s.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  add_as_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_dspaddsubsat(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_AS_FR2X16 */

#if (!defined(__DEFINED_ADD_SA_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_SA_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_SA_FR2X16))))

#define __DEFINED_ADD_SA_FR2X16

/* Performs a vector subtract/add on the two input fract2x16s.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  add_sa_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_dspsubaddsat(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_SA_FR2X16 */

#if (!defined(__DEFINED_ADD_ON_SIGN) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_ON_SIGN)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_ON_SIGN))))

#define __DEFINED_ADD_ON_SIGN

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  add_on_sign(int  __a, int  __b) {
  int  __rval = __builtin_add_on_sign(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_ON_SIGN */

#if (!defined(__DEFINED_SEARCH_GT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SEARCH_GT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SEARCH_GT))))

#define __DEFINED_SEARCH_GT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int * search_gt(int  __a, int * __b, int * __c, int * __d, short  __e, short  __f, int * *__r2, short  *__r3, short  *__r4) {
  int * __rval = __builtin_search_gt_r1(__a, __b, __c, __d, __e, __f);
  *__r2 = __builtin_search_gt_r2(__rval);
  *__r3 = __builtin_search_gt_r3(__rval);
  *__r4 = __builtin_search_gt_r4(__rval);
  return __rval;
}

#endif /* __DEFINED_SEARCH_GT */

#if (!defined(__DEFINED_SEARCH_GE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SEARCH_GE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SEARCH_GE))))

#define __DEFINED_SEARCH_GE

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int * search_ge(int  __a, int * __b, int * __c, int * __d, short  __e, short  __f, int * *__r2, short  *__r3, short  *__r4) {
  int * __rval = __builtin_search_ge_r1(__a, __b, __c, __d, __e, __f);
  *__r2 = __builtin_search_ge_r2(__rval);
  *__r3 = __builtin_search_ge_r3(__rval);
  *__r4 = __builtin_search_ge_r4(__rval);
  return __rval;
}

#endif /* __DEFINED_SEARCH_GE */

#if (!defined(__DEFINED_SEARCH_LE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SEARCH_LE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SEARCH_LE))))

#define __DEFINED_SEARCH_LE

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int * search_le(int  __a, int * __b, int * __c, int * __d, short  __e, short  __f, int * *__r2, short  *__r3, short  *__r4) {
  int * __rval = __builtin_search_le_r1(__a, __b, __c, __d, __e, __f);
  *__r2 = __builtin_search_le_r2(__rval);
  *__r3 = __builtin_search_le_r3(__rval);
  *__r4 = __builtin_search_le_r4(__rval);
  return __rval;
}

#endif /* __DEFINED_SEARCH_LE */

#if (!defined(__DEFINED_SEARCH_LT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SEARCH_LT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SEARCH_LT))))

#define __DEFINED_SEARCH_LT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int * search_lt(int  __a, int * __b, int * __c, int * __d, short  __e, short  __f, int * *__r2, short  *__r3, short  *__r4) {
  int * __rval = __builtin_search_lt_r1(__a, __b, __c, __d, __e, __f);
  *__r2 = __builtin_search_lt_r2(__rval);
  *__r3 = __builtin_search_lt_r3(__rval);
  *__r4 = __builtin_search_lt_r4(__rval);
  return __rval;
}

#endif /* __DEFINED_SEARCH_LT */

#if (!defined(__DEFINED_MULTR_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTR_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTR_FR2X16))))

#define __DEFINED_MULTR_FR2X16

/* Multiplies two packed fracts.  Rounds the result to 16 bits.
** Whether the rounding is biased or unbiased depends on what
** the RND_MOD bit in the ASTAT register is set to.
** Input: __a{a,b} __b{c,d}
** Returns: {round16{a*c},round16{b*d}}
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  multr_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_multr_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTR_FR2X16 */

#if (!defined(__DEFINED_MULT_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_I2X16))))

#define __DEFINED_MULT_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  mult_i2x16(int  __a, int  __b) {
  int  __rval = __builtin_mult_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_I2X16 */

#if (!defined(__DEFINED_MULT_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_FR2X16))))

#define __DEFINED_MULT_FR2X16

/* Multiplies two packed fracts. Truncates the results to 16 bits.
** Inputs: __a{a,b} __b{c,d}
** Returns: {trunc16(a*c),trunc16(b*d)}
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  mult_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_mult_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_FR2X16 */

#if (!defined(__DEFINED_MULTU_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTU_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTU_FR2X16))))

#define __DEFINED_MULTU_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  multu_fr2x16(int  __a, int  __b) {
  int  __rval = __builtin_multu_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTU_FR2X16 */

#if (!defined(__DEFINED_ALIGN8) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ALIGN8)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ALIGN8))))

#define __DEFINED_ALIGN8

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  align8(int  __a, int  __b) {
  int  __rval = __builtin_align8(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ALIGN8 */

#if (!defined(__DEFINED_ALIGN16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ALIGN16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ALIGN16))))

#define __DEFINED_ALIGN16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  align16(int  __a, int  __b) {
  int  __rval = __builtin_align16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ALIGN16 */

#if (!defined(__DEFINED_ALIGN24) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ALIGN24)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ALIGN24))))

#define __DEFINED_ALIGN24

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  align24(int  __a, int  __b) {
  int  __rval = __builtin_align24(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ALIGN24 */

#if (!defined(__DEFINED_LOADBYTES) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_LOADBYTES)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_LOADBYTES))))

#define __DEFINED_LOADBYTES

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  loadbytes(int * __a) {
  int  __rval = __builtin_loadbytes(__a);
  return __rval;
}

#endif /* __DEFINED_LOADBYTES */

#if (!defined(__DEFINED_BYTEPACK) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BYTEPACK)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BYTEPACK))))

#define __DEFINED_BYTEPACK

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bytepack(int  __a, int  __b) {
  int  __rval = __builtin_bytepack(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BYTEPACK */

#if (!defined(__DEFINED_NOP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_NOP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_NOP))))

#define __DEFINED_NOP

/* Insert a normal 16 bit NOP, which is treated as volatile.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  NOP(void) {
  __builtin_NOP();
}

#endif /* __DEFINED_NOP */

#if (!defined(__DEFINED_CLI) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CLI)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CLI))))

#define __DEFINED_CLI

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned int  cli(void) {
  unsigned int  __rval = __builtin_cli();
  return __rval;
}

#endif /* __DEFINED_CLI */

#if (!defined(__DEFINED_STI) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_STI)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_STI))))

#define __DEFINED_STI

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  sti(unsigned int  __a) {
  __builtin_sti(__a);
}

#endif /* __DEFINED_STI */

#if (!defined(__DEFINED_CSYNC_INT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CSYNC_INT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CSYNC_INT))))

#define __DEFINED_CSYNC_INT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  csync_int(void) {
  __builtin_csync_int();
}

#endif /* __DEFINED_CSYNC_INT */

#if (!defined(__DEFINED_SSYNC_INT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SSYNC_INT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SSYNC_INT))))

#define __DEFINED_SSYNC_INT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  ssync_int(void) {
  __builtin_ssync_int();
}

#endif /* __DEFINED_SSYNC_INT */

#if (!defined(__DEFINED_IDLE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_IDLE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_IDLE))))

#define __DEFINED_IDLE

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  idle(void) {
  __builtin_idle();
}

#endif /* __DEFINED_IDLE */

#if (!defined(__DEFINED_HALT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_HALT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_HALT))))

#define __DEFINED_HALT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  halt(void) {
  __builtin_halt();
}

#endif /* __DEFINED_HALT */

#if (!defined(__DEFINED_ABORT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ABORT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ABORT))))

#define __DEFINED_ABORT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  abort(void) {
  __builtin_abort();
}

#endif /* __DEFINED_ABORT */

#if (!defined(__DEFINED_RAISE_INTR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_RAISE_INTR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_RAISE_INTR))))

#define __DEFINED_RAISE_INTR

#define raise_intr(A) (__builtin_raise((A)))

#endif /* __DEFINED_RAISE_INTR */

#if (!defined(__DEFINED_EXCPT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EXCPT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EXCPT))))

#define __DEFINED_EXCPT

#define excpt(A) (__builtin_excpt((A)))

#endif /* __DEFINED_EXCPT */

#if (!defined(__DEFINED_TESTSET) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_TESTSET)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_TESTSET))))

#define __DEFINED_TESTSET

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  testset(char * __a) {
  int  __rval = __builtin_testset(__a);
  return __rval;
}

#endif /* __DEFINED_TESTSET */

#if (!defined(__DEFINED_PREFETCH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_PREFETCH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_PREFETCH))))

#define __DEFINED_PREFETCH

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  prefetch(void * __a) {
  __builtin_prefetch(__a);
}

#endif /* __DEFINED_PREFETCH */

#if (!defined(__DEFINED_PREFETCHMODUP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_PREFETCHMODUP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_PREFETCHMODUP))))

#define __DEFINED_PREFETCHMODUP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * prefetchmodup(void * __a) {
  void * __rval = __builtin_prefetchmodup(__a);
  return __rval;
}

#endif /* __DEFINED_PREFETCHMODUP */

#if (!defined(__DEFINED_FLUSHINV) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_FLUSHINV)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_FLUSHINV))))

#define __DEFINED_FLUSHINV

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  flushinv(void * __a) {
  __builtin_flushinv(__a);
}

#endif /* __DEFINED_FLUSHINV */

#if (!defined(__DEFINED_FLUSHINVMODUP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_FLUSHINVMODUP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_FLUSHINVMODUP))))

#define __DEFINED_FLUSHINVMODUP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * flushinvmodup(void * __a) {
  void * __rval = __builtin_flushinvmodup(__a);
  return __rval;
}

#endif /* __DEFINED_FLUSHINVMODUP */

#if (!defined(__DEFINED_FLUSH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_FLUSH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_FLUSH))))

#define __DEFINED_FLUSH

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  flush(void * __a) {
  __builtin_flush(__a);
}

#endif /* __DEFINED_FLUSH */

#if (!defined(__DEFINED_FLUSHMODUP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_FLUSHMODUP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_FLUSHMODUP))))

#define __DEFINED_FLUSHMODUP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * flushmodup(void * __a) {
  void * __rval = __builtin_flushmodup(__a);
  return __rval;
}

#endif /* __DEFINED_FLUSHMODUP */

#if (!defined(__DEFINED_IFLUSH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_IFLUSH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_IFLUSH))))

#define __DEFINED_IFLUSH

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  iflush(void * __a) {
  __builtin_iflush(__a);
}

#endif /* __DEFINED_IFLUSH */

#if (!defined(__DEFINED_IFLUSHMODUP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_IFLUSHMODUP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_IFLUSHMODUP))))

#define __DEFINED_IFLUSHMODUP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * iflushmodup(void * __a) {
  void * __rval = __builtin_iflushmodup(__a);
  return __rval;
}

#endif /* __DEFINED_IFLUSHMODUP */

#if (!defined(__DEFINED_BFIN_IFLUSHMODUP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_IFLUSHMODUP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_IFLUSHMODUP))))

#define __DEFINED_BFIN_IFLUSHMODUP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * bfin_iflushmodup(void * __a) {
  void * __rval = __builtin_bfin_iflushmodup(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_IFLUSHMODUP */

#if (!defined(__DEFINED_BFIN_IFLUSH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_IFLUSH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_IFLUSH))))

#define __DEFINED_BFIN_IFLUSH

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_iflush(void * __a) {
  __builtin_bfin_iflush(__a);
}

#endif /* __DEFINED_BFIN_IFLUSH */

#if (!defined(__DEFINED_BFIN_FLUSHMODUP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_FLUSHMODUP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_FLUSHMODUP))))

#define __DEFINED_BFIN_FLUSHMODUP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * bfin_flushmodup(void * __a) {
  void * __rval = __builtin_bfin_flushmodup(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_FLUSHMODUP */

#if (!defined(__DEFINED_BFIN_FLUSH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_FLUSH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_FLUSH))))

#define __DEFINED_BFIN_FLUSH

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_flush(void * __a) {
  __builtin_bfin_flush(__a);
}

#endif /* __DEFINED_BFIN_FLUSH */

#if (!defined(__DEFINED_BFIN_FLUSHINVMODUP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_FLUSHINVMODUP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_FLUSHINVMODUP))))

#define __DEFINED_BFIN_FLUSHINVMODUP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * bfin_flushinvmodup(void * __a) {
  void * __rval = __builtin_bfin_flushinvmodup(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_FLUSHINVMODUP */

#if (!defined(__DEFINED_BFIN_FLUSHINV) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_FLUSHINV)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_FLUSHINV))))

#define __DEFINED_BFIN_FLUSHINV

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_flushinv(void * __a) {
  __builtin_bfin_flushinv(__a);
}

#endif /* __DEFINED_BFIN_FLUSHINV */

#if (!defined(__DEFINED_BFIN_PREFETCHMODUP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_PREFETCHMODUP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_PREFETCHMODUP))))

#define __DEFINED_BFIN_PREFETCHMODUP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void * bfin_prefetchmodup(void * __a) {
  void * __rval = __builtin_bfin_prefetchmodup(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_PREFETCHMODUP */

#if (!defined(__DEFINED_BFIN_PREFETCH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_PREFETCH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_PREFETCH))))

#define __DEFINED_BFIN_PREFETCH

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_prefetch(void * __a) {
  __builtin_bfin_prefetch(__a);
}

#endif /* __DEFINED_BFIN_PREFETCH */

#if (!defined(__DEFINED_BFIN_TESTSET) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_TESTSET)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_TESTSET))))

#define __DEFINED_BFIN_TESTSET

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_testset(char * __a) {
  int  __rval = __builtin_bfin_testset(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_TESTSET */

#if (!defined(__DEFINED_BFIN_EXCPT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_EXCPT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_EXCPT))))

#define __DEFINED_BFIN_EXCPT

#define bfin_excpt(A) (__builtin_bfin_excpt((A)))

#endif /* __DEFINED_BFIN_EXCPT */

#if (!defined(__DEFINED_BFIN_RAISE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_RAISE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_RAISE))))

#define __DEFINED_BFIN_RAISE

#define bfin_raise(A) (__builtin_bfin_raise((A)))

#endif /* __DEFINED_BFIN_RAISE */

#if (!defined(__DEFINED_BFIN_ABORT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ABORT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ABORT))))

#define __DEFINED_BFIN_ABORT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_abort(void) {
  __builtin_bfin_abort();
}

#endif /* __DEFINED_BFIN_ABORT */

#if (!defined(__DEFINED_BFIN_HALT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_HALT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_HALT))))

#define __DEFINED_BFIN_HALT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_halt(void) {
  __builtin_bfin_halt();
}

#endif /* __DEFINED_BFIN_HALT */

#if (!defined(__DEFINED_BFIN_IDLE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_IDLE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_IDLE))))

#define __DEFINED_BFIN_IDLE

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_idle(void) {
  __builtin_bfin_idle();
}

#endif /* __DEFINED_BFIN_IDLE */

#if (!defined(__DEFINED_BFIN_SSYNC_INT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SSYNC_INT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SSYNC_INT))))

#define __DEFINED_BFIN_SSYNC_INT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_ssync_int(void) {
  __builtin_bfin_ssync_int();
}

#endif /* __DEFINED_BFIN_SSYNC_INT */

#if (!defined(__DEFINED_BFIN_CSYNC_INT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CSYNC_INT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CSYNC_INT))))

#define __DEFINED_BFIN_CSYNC_INT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_csync_int(void) {
  __builtin_bfin_csync_int();
}

#endif /* __DEFINED_BFIN_CSYNC_INT */

#if (!defined(__DEFINED_BFIN_STI) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_STI)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_STI))))

#define __DEFINED_BFIN_STI

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_sti(unsigned int  __a) {
  __builtin_bfin_sti(__a);
}

#endif /* __DEFINED_BFIN_STI */

#if (!defined(__DEFINED_BFIN_CLI) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CLI)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CLI))))

#define __DEFINED_BFIN_CLI

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned int  bfin_cli(void) {
  unsigned int  __rval = __builtin_bfin_cli();
  return __rval;
}

#endif /* __DEFINED_BFIN_CLI */

#if (!defined(__DEFINED_BFIN_BYTEPACK) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_BYTEPACK)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_BYTEPACK))))

#define __DEFINED_BFIN_BYTEPACK

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_bytepack(int  __a, int  __b) {
  int  __rval = __builtin_bfin_bytepack(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_BYTEPACK */

#if (!defined(__DEFINED_BFIN_LOADBYTES) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_LOADBYTES)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_LOADBYTES))))

#define __DEFINED_BFIN_LOADBYTES

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_loadbytes(int * __a) {
  int  __rval = __builtin_bfin_loadbytes(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_LOADBYTES */

#if (!defined(__DEFINED_BFIN_ALIGN24) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ALIGN24)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ALIGN24))))

#define __DEFINED_BFIN_ALIGN24

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_align24(int  __a, int  __b) {
  int  __rval = __builtin_bfin_align24(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_ALIGN24 */

#if (!defined(__DEFINED_BFIN_ALIGN16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ALIGN16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ALIGN16))))

#define __DEFINED_BFIN_ALIGN16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_align16(int  __a, int  __b) {
  int  __rval = __builtin_bfin_align16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_ALIGN16 */

#if (!defined(__DEFINED_BFIN_ALIGN8) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ALIGN8)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ALIGN8))))

#define __DEFINED_BFIN_ALIGN8

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_align8(int  __a, int  __b) {
  int  __rval = __builtin_bfin_align8(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_ALIGN8 */

#if (!defined(__DEFINED_BFIN_MULTU_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULTU_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULTU_FR2X16))))

#define __DEFINED_BFIN_MULTU_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_multu_fr2x16(int  __a, int  __b) {
  int  __rval = __builtin_bfin_multu_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULTU_FR2X16 */

#if (!defined(__DEFINED_BFIN_MULT_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULT_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULT_FR2X16))))

#define __DEFINED_BFIN_MULT_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_mult_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_bfin_mult_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULT_FR2X16 */

#if (!defined(__DEFINED_BFIN_MULT_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULT_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULT_I2X16))))

#define __DEFINED_BFIN_MULT_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_mult_i2x16(int  __a, int  __b) {
  int  __rval = __builtin_bfin_mult_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULT_I2X16 */

#if (!defined(__DEFINED_BFIN_MULTR_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULTR_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULTR_FR2X16))))

#define __DEFINED_BFIN_MULTR_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_multr_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_bfin_multr_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULTR_FR2X16 */

#if (!defined(__DEFINED_BFIN_DSPSUBADDSAT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_DSPSUBADDSAT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_DSPSUBADDSAT))))

#define __DEFINED_BFIN_DSPSUBADDSAT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_dspsubaddsat(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_bfin_dspsubaddsat(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_DSPSUBADDSAT */

#if (!defined(__DEFINED_BFIN_DSPADDSUBSAT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_DSPADDSUBSAT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_DSPADDSUBSAT))))

#define __DEFINED_BFIN_DSPADDSUBSAT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_dspaddsubsat(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_bfin_dspaddsubsat(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_DSPADDSUBSAT */

#if (!defined(__DEFINED_BFIN_SUB_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SUB_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SUB_FR2X16))))

#define __DEFINED_BFIN_SUB_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_sub_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_bfin_sub_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SUB_FR2X16 */

#if (!defined(__DEFINED_BFIN_SUB_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SUB_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SUB_I2X16))))

#define __DEFINED_BFIN_SUB_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_sub_i2x16(int  __a, int  __b) {
  int  __rval = __builtin_bfin_sub_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SUB_I2X16 */

#if (!defined(__DEFINED_BFIN_ADD_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ADD_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ADD_FR2X16))))

#define __DEFINED_BFIN_ADD_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_add_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_bfin_add_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_ADD_FR2X16 */

#if (!defined(__DEFINED_BFIN_ADD_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ADD_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ADD_I2X16))))

#define __DEFINED_BFIN_ADD_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_add_i2x16(int  __a, int  __b) {
  int  __rval = __builtin_bfin_add_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_ADD_I2X16 */

#if (!defined(__DEFINED_BFIN_A_MSU_M) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MSU_M)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MSU_M))))

#define __DEFINED_BFIN_A_MSU_M

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_msu_M(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_bfin_A_msu_M(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MSU_M */

#if (!defined(__DEFINED_BFIN_A_MAC_M) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MAC_M)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MAC_M))))

#define __DEFINED_BFIN_A_MAC_M

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mac_M(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_bfin_A_mac_M(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MAC_M */

#if (!defined(__DEFINED_BFIN_A_MULT_M) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MULT_M)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MULT_M))))

#define __DEFINED_BFIN_A_MULT_M

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mult_M(fract16  __a, fract16  __b) {
  acc40  __rval = __builtin_bfin_A_mult_M(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MULT_M */

#if (!defined(__DEFINED_BFIN_A_MSU_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MSU_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MSU_FU))))

#define __DEFINED_BFIN_A_MSU_FU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_msu_FU(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_bfin_A_msu_FU(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MSU_FU */

#if (!defined(__DEFINED_BFIN_A_MAC_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MAC_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MAC_FU))))

#define __DEFINED_BFIN_A_MAC_FU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mac_FU(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_bfin_A_mac_FU(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MAC_FU */

#if (!defined(__DEFINED_BFIN_A_MULT_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MULT_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MULT_FU))))

#define __DEFINED_BFIN_A_MULT_FU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mult_FU(fract16  __a, fract16  __b) {
  acc40  __rval = __builtin_bfin_A_mult_FU(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MULT_FU */

#if (!defined(__DEFINED_BFIN_A_MSU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MSU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MSU))))

#define __DEFINED_BFIN_A_MSU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_msu(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_bfin_A_msu(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MSU */

#if (!defined(__DEFINED_BFIN_A_MAC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MAC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MAC))))

#define __DEFINED_BFIN_A_MAC

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mac(acc40  __a, fract16  __b, fract16  __c) {
  acc40  __rval = __builtin_bfin_A_mac(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MAC */

#if (!defined(__DEFINED_BFIN_A_MULT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MULT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MULT))))

#define __DEFINED_BFIN_A_MULT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mult(fract16  __a, fract16  __b) {
  acc40  __rval = __builtin_bfin_A_mult(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MULT */

#if (!defined(__DEFINED_BFIN_A_MSU_MIS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MSU_MIS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MSU_MIS))))

#define __DEFINED_BFIN_A_MSU_MIS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_msu_MIS(acc40  __a, short  __b, unsigned short  __c) {
  acc40  __rval = __builtin_bfin_A_msu_MIS(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MSU_MIS */

#if (!defined(__DEFINED_BFIN_A_MAC_MIS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MAC_MIS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MAC_MIS))))

#define __DEFINED_BFIN_A_MAC_MIS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mac_MIS(acc40  __a, short  __b, unsigned short  __c) {
  acc40  __rval = __builtin_bfin_A_mac_MIS(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MAC_MIS */

#if (!defined(__DEFINED_BFIN_A_MULT_MIS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MULT_MIS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MULT_MIS))))

#define __DEFINED_BFIN_A_MULT_MIS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mult_MIS(short  __a, unsigned short  __b) {
  acc40  __rval = __builtin_bfin_A_mult_MIS(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MULT_MIS */

#if (!defined(__DEFINED_BFIN_A_MSU_IS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MSU_IS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MSU_IS))))

#define __DEFINED_BFIN_A_MSU_IS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_msu_IS(acc40  __a, short  __b, short  __c) {
  acc40  __rval = __builtin_bfin_A_msu_IS(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MSU_IS */

#if (!defined(__DEFINED_BFIN_A_MAC_IS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MAC_IS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MAC_IS))))

#define __DEFINED_BFIN_A_MAC_IS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mac_IS(acc40  __a, short  __b, short  __c) {
  acc40  __rval = __builtin_bfin_A_mac_IS(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MAC_IS */

#if (!defined(__DEFINED_BFIN_A_MULT_IS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MULT_IS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MULT_IS))))

#define __DEFINED_BFIN_A_MULT_IS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_mult_IS(short  __a, short  __b) {
  acc40  __rval = __builtin_bfin_A_mult_IS(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MULT_IS */

#if (!defined(__DEFINED_BFIN_CSQUMAG_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CSQUMAG_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CSQUMAG_FR32))))

#define __DEFINED_BFIN_CSQUMAG_FR32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_csqumag_fr32(int  __a) {
  int  __rval = __builtin_bfin_csqumag_fr32(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_CSQUMAG_FR32 */

#if (!defined(__DEFINED_BFIN_CSQUMAG_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CSQUMAG_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CSQUMAG_FR16))))

#define __DEFINED_BFIN_CSQUMAG_FR16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_csqumag_fr16(int  __a) {
  short  __rval = __builtin_bfin_csqumag_fr16(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_CSQUMAG_FR16 */

#if (!defined(__DEFINED_BFIN_CMPLX_CONJ_MUL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_CONJ_MUL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_CONJ_MUL))))

#define __DEFINED_BFIN_CMPLX_CONJ_MUL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_cmplx_conj_mul(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_bfin_cmplx_conj_mul(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_CONJ_MUL */

#if (!defined(__DEFINED_BFIN_CMPLX_MUL) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_MUL)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_MUL))))

#define __DEFINED_BFIN_CMPLX_MUL

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_cmplx_mul(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_bfin_cmplx_mul(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_MUL */

#if (!defined(__DEFINED_BFIN_MULTM_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULTM_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULTM_FR1X16))))

#define __DEFINED_BFIN_MULTM_FR1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_multm_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_bfin_multm_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULTM_FR1X16 */

#if (!defined(__DEFINED_BFIN_MULTMR_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULTMR_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULTMR_FR1X16))))

#define __DEFINED_BFIN_MULTMR_FR1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_multmr_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_bfin_multmr_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULTMR_FR1X16 */

#if (!defined(__DEFINED_BFIN_MULTR_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULTR_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULTR_FR1X16))))

#define __DEFINED_BFIN_MULTR_FR1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_multr_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_bfin_multr_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULTR_FR1X16 */

#if (!defined(__DEFINED_BFIN_A_SIGNBITS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_SIGNBITS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_SIGNBITS))))

#define __DEFINED_BFIN_A_SIGNBITS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_A_signbits(acc40  __a) {
  short  __rval = __builtin_bfin_A_signbits(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_SIGNBITS */

#if (!defined(__DEFINED_BFIN_NORM_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_NORM_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_NORM_FR1X16))))

#define __DEFINED_BFIN_NORM_FR1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_norm_fr1x16(fract16  __a) {
  short  __rval = __builtin_bfin_norm_fr1x16(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_NORM_FR1X16 */

#if (!defined(__DEFINED_BFIN_NORM_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_NORM_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_NORM_FR1X32))))

#define __DEFINED_BFIN_NORM_FR1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_norm_fr1x32(fract32  __a) {
  short  __rval = __builtin_bfin_norm_fr1x32(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_NORM_FR1X32 */

#if (!defined(__DEFINED_BFIN_ROUND_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ROUND_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ROUND_FR1X32))))

#define __DEFINED_BFIN_ROUND_FR1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_round_fr1x32(fract32  __a) {
  fract16  __rval = __builtin_bfin_round_fr1x32(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_ROUND_FR1X32 */

#if (!defined(__DEFINED_BFIN_NEGATE_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_NEGATE_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_NEGATE_FR2X16))))

#define __DEFINED_BFIN_NEGATE_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_negate_fr2x16(fract2x16  __a) {
  fract2x16  __rval = __builtin_bfin_negate_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_NEGATE_FR2X16 */

#if (!defined(__DEFINED_BFIN_A_NEG) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_NEG)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_NEG))))

#define __DEFINED_BFIN_A_NEG

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_neg(acc40  __a) {
  acc40  __rval = __builtin_bfin_A_neg(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_NEG */

#if (!defined(__DEFINED_BFIN_NEGATE_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_NEGATE_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_NEGATE_FR1X32))))

#define __DEFINED_BFIN_NEGATE_FR1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_negate_fr1x32(fract32  __a) {
  fract32  __rval = __builtin_bfin_negate_fr1x32(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_NEGATE_FR1X32 */

#if (!defined(__DEFINED_BFIN_MIN_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MIN_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MIN_FR2X16))))

#define __DEFINED_BFIN_MIN_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_min_fr2x16(fract2x16  __a, fract2x16  __b) {
  fract2x16  __rval = __builtin_bfin_min_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MIN_FR2X16 */

#if (!defined(__DEFINED_BFIN_MAX_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MAX_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MAX_I2X16))))

#define __DEFINED_BFIN_MAX_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_max_i2x16(int  __a, int  __b) {
  int  __rval = __builtin_bfin_max_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MAX_I2X16 */

#if (!defined(__DEFINED_BFIN_RVITMAX2X16_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_RVITMAX2X16_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_RVITMAX2X16_R1))))

#define __DEFINED_BFIN_RVITMAX2X16_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_rvitmax2x16_r1(int  __a, int  __b, int  __c, int  *__r2) {
  int  __rval = __builtin_bfin_rvitmax2x16_r1(__a, __b, __c);
  *__r2 = __builtin_rvitmax2x16_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_RVITMAX2X16_R1 */

#if (!defined(__DEFINED_BFIN_LVITMAX2X16_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_LVITMAX2X16_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_LVITMAX2X16_R1))))

#define __DEFINED_BFIN_LVITMAX2X16_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_lvitmax2x16_r1(int  __a, int  __b, int  __c, int  *__r2) {
  int  __rval = __builtin_bfin_lvitmax2x16_r1(__a, __b, __c);
  *__r2 = __builtin_lvitmax2x16_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_LVITMAX2X16_R1 */

#if (!defined(__DEFINED_BFIN_RVITMAX1X16_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_RVITMAX1X16_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_RVITMAX1X16_R1))))

#define __DEFINED_BFIN_RVITMAX1X16_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_rvitmax1x16_r1(int  __a, int  __b, int  *__r2) {
  short  __rval = __builtin_bfin_rvitmax1x16_r1(__a, __b);
  *__r2 = __builtin_rvitmax1x16_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_RVITMAX1X16_R1 */

#if (!defined(__DEFINED_BFIN_LVITMAX1X16_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_LVITMAX1X16_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_LVITMAX1X16_R1))))

#define __DEFINED_BFIN_LVITMAX1X16_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_lvitmax1x16_r1(int  __a, int  __b, int  *__r2) {
  short  __rval = __builtin_bfin_lvitmax1x16_r1(__a, __b);
  *__r2 = __builtin_lvitmax1x16_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_LVITMAX1X16_R1 */

#if (!defined(__DEFINED_BFIN_EXPADJ2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_EXPADJ2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_EXPADJ2X16))))

#define __DEFINED_BFIN_EXPADJ2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_expadj2x16(int  __a, short  __b) {
  short  __rval = __builtin_bfin_expadj2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_EXPADJ2X16 */

#if (!defined(__DEFINED_BFIN_EXPADJ1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_EXPADJ1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_EXPADJ1X16))))

#define __DEFINED_BFIN_EXPADJ1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_expadj1x16(short  __a, short  __b) {
  short  __rval = __builtin_bfin_expadj1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_EXPADJ1X16 */

#if (!defined(__DEFINED_BFIN_EXPADJ1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_EXPADJ1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_EXPADJ1X32))))

#define __DEFINED_BFIN_EXPADJ1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_expadj1x32(int  __a, short  __b) {
  short  __rval = __builtin_bfin_expadj1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_EXPADJ1X32 */

#if (!defined(__DEFINED_BFIN_DIVQ_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_DIVQ_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_DIVQ_R1))))

#define __DEFINED_BFIN_DIVQ_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_divq_r1(int  __a, int  __b, int  __c, int  *__r2) {
  int  __rval = __builtin_bfin_divq_r1(__a, __b, __c);
  *__r2 = __builtin_divq_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_DIVQ_R1 */

#if (!defined(__DEFINED_BFIN_DIVS_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_DIVS_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_DIVS_R1))))

#define __DEFINED_BFIN_DIVS_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_divs_r1(int  __a, int  __b, int  *__r2) {
  int  __rval = __builtin_bfin_divs_r1(__a, __b);
  *__r2 = __builtin_divs_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_DIVS_R1 */

#if (!defined(__DEFINED_BFIN_A_ADD) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_ADD)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_ADD))))

#define __DEFINED_BFIN_A_ADD

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_add(acc40  __a, acc40  __b) {
  acc40  __rval = __builtin_bfin_A_add(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_ADD */

#if (!defined(__DEFINED_BFIN_A_SUB) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_SUB)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_SUB))))

#define __DEFINED_BFIN_A_SUB

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_sub(acc40  __a, acc40  __b) {
  acc40  __rval = __builtin_bfin_A_sub(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_SUB */

#if (!defined(__DEFINED_BFIN_ABS_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ABS_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ABS_FR2X16))))

#define __DEFINED_BFIN_ABS_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_abs_fr2x16(fract2x16  __a) {
  fract2x16  __rval = __builtin_bfin_abs_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_ABS_FR2X16 */

#if (!defined(__DEFINED_BFIN_A_ABS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_ABS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_ABS))))

#define __DEFINED_BFIN_A_ABS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_abs(acc40  __a) {
  acc40  __rval = __builtin_bfin_A_abs(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_ABS */

#if (!defined(__DEFINED_BFIN_ABS_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ABS_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ABS_FR1X32))))

#define __DEFINED_BFIN_ABS_FR1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_abs_fr1x32(fract32  __a) {
  fract32  __rval = __builtin_bfin_abs_fr1x32(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_ABS_FR1X32 */

#if (!defined(__DEFINED_BFIN_A_LSHIFT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_LSHIFT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_LSHIFT))))

#define __DEFINED_BFIN_A_LSHIFT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_lshift(acc40  __a, short  __b) {
  acc40  __rval = __builtin_bfin_A_lshift(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_LSHIFT */

#if (!defined(__DEFINED_BFIN_SHLL_I1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHLL_I1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHLL_I1X32))))

#define __DEFINED_BFIN_SHLL_I1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_shll_i1x32(int  __a, short  __b) {
  int  __rval = __builtin_bfin_shll_i1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHLL_I1X32 */

#if (!defined(__DEFINED_BFIN_SHLL_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHLL_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHLL_I2X16))))

#define __DEFINED_BFIN_SHLL_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_shll_i2x16(int  __a, short  __b) {
  int  __rval = __builtin_bfin_shll_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHLL_I2X16 */

#if (!defined(__DEFINED_BFIN_SHLL_I1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHLL_I1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHLL_I1X16))))

#define __DEFINED_BFIN_SHLL_I1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_shll_i1x16(short  __a, short  __b) {
  short  __rval = __builtin_bfin_shll_i1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHLL_I1X16 */

#if (!defined(__DEFINED_BFIN_A_ASHIFT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_ASHIFT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_ASHIFT))))

#define __DEFINED_BFIN_A_ASHIFT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_ashift(acc40  __a, short  __b) {
  acc40  __rval = __builtin_bfin_A_ashift(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_ASHIFT */

#if (!defined(__DEFINED_BFIN_SHL_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHL_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHL_FR1X32))))

#define __DEFINED_BFIN_SHL_FR1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_shl_fr1x32(fract32  __a, short  __b) {
  fract32  __rval = __builtin_bfin_shl_fr1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHL_FR1X32 */

#if (!defined(__DEFINED_BFIN_SHL_I1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHL_I1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHL_I1X32))))

#define __DEFINED_BFIN_SHL_I1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_shl_i1x32(int  __a, short  __b) {
  int  __rval = __builtin_bfin_shl_i1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHL_I1X32 */

#if (!defined(__DEFINED_BFIN_SHL_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHL_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHL_FR2X16))))

#define __DEFINED_BFIN_SHL_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_shl_fr2x16(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_bfin_shl_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHL_FR2X16 */

#if (!defined(__DEFINED_BFIN_SHL_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHL_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHL_FR1X16))))

#define __DEFINED_BFIN_SHL_FR1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_shl_fr1x16(fract16  __a, short  __b) {
  fract16  __rval = __builtin_bfin_shl_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHL_FR1X16 */

#if (!defined(__DEFINED_BFIN_SHL_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHL_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHL_I2X16))))

#define __DEFINED_BFIN_SHL_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_shl_i2x16(int  __a, short  __b) {
  int  __rval = __builtin_bfin_shl_i2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHL_I2X16 */

#if (!defined(__DEFINED_BFIN_SHL_I1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHL_I1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHL_I1X16))))

#define __DEFINED_BFIN_SHL_I1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_shl_i1x16(short  __a, short  __b) {
  short  __rval = __builtin_bfin_shl_i1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHL_I1X16 */

#if (!defined(__DEFINED_BFIN_A_BITMUX_ASR_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_BITMUX_ASR_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_BITMUX_ASR_R1))))

#define __DEFINED_BFIN_A_BITMUX_ASR_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_A_bitmux_ASR_r1(int  __a, int  __b, acc40  __c, int  *__r2, acc40  *__r3) {
  int  __rval = __builtin_bfin_A_bitmux_ASR_r1(__a, __b, __c);
  *__r2 = __builtin_A_bitmux_ASR_r2(__rval);
  *__r3 = __builtin_A_bitmux_ASR_r3(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_BITMUX_ASR_R1 */

#if (!defined(__DEFINED_BFIN_A_BITMUX_ASL_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_BITMUX_ASL_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_BITMUX_ASL_R1))))

#define __DEFINED_BFIN_A_BITMUX_ASL_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_A_bitmux_ASL_r1(int  __a, int  __b, acc40  __c, int  *__r2, acc40  *__r3) {
  int  __rval = __builtin_bfin_A_bitmux_ASL_r1(__a, __b, __c);
  *__r2 = __builtin_A_bitmux_ASL_r2(__rval);
  *__r3 = __builtin_A_bitmux_ASL_r3(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_BITMUX_ASL_R1 */

#if (!defined(__DEFINED_BFIN_ONES) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ONES)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ONES))))

#define __DEFINED_BFIN_ONES

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_ones(int  __a) {
  short  __rval = __builtin_bfin_ones(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_ONES */

#if (!defined(__DEFINED_BFIN_A_LE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_LE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_LE))))

#define __DEFINED_BFIN_A_LE

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_A_le(acc40  __a, acc40  __b) {
  int  __rval = __builtin_bfin_A_le(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_LE */

#if (!defined(__DEFINED_BFIN_A_LT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_LT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_LT))))

#define __DEFINED_BFIN_A_LT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_A_lt(acc40  __a, acc40  __b) {
  int  __rval = __builtin_bfin_A_lt(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_LT */

#if (!defined(__DEFINED_BFIN_A_EQ) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_EQ)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_EQ))))

#define __DEFINED_BFIN_A_EQ

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_A_eq(acc40  __a, acc40  __b) {
  int  __rval = __builtin_bfin_A_eq(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_EQ */

#if (!defined(__DEFINED_BFIN_A_MADH_IH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MADH_IH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MADH_IH))))

#define __DEFINED_BFIN_A_MADH_IH

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_A_madh_IH(acc40  __a) {
  short  __rval = __builtin_bfin_A_madh_IH(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MADH_IH */

#if (!defined(__DEFINED_BFIN_A_MADH_ISS2) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MADH_ISS2)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MADH_ISS2))))

#define __DEFINED_BFIN_A_MADH_ISS2

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_A_madh_ISS2(acc40  __a) {
  short  __rval = __builtin_bfin_A_madh_ISS2(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MADH_ISS2 */

#if (!defined(__DEFINED_BFIN_A_MADH_S2RND) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MADH_S2RND)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MADH_S2RND))))

#define __DEFINED_BFIN_A_MADH_S2RND

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_A_madh_S2RND(acc40  __a) {
  fract16  __rval = __builtin_bfin_A_madh_S2RND(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MADH_S2RND */

#if (!defined(__DEFINED_BFIN_A_MADH_TFU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MADH_TFU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MADH_TFU))))

#define __DEFINED_BFIN_A_MADH_TFU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_A_madh_TFU(acc40  __a) {
  fract16  __rval = __builtin_bfin_A_madh_TFU(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MADH_TFU */

#if (!defined(__DEFINED_BFIN_A_MADH_T) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MADH_T)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MADH_T))))

#define __DEFINED_BFIN_A_MADH_T

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_A_madh_T(acc40  __a) {
  fract16  __rval = __builtin_bfin_A_madh_T(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MADH_T */

#if (!defined(__DEFINED_BFIN_A_MADH_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MADH_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MADH_FU))))

#define __DEFINED_BFIN_A_MADH_FU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_A_madh_FU(acc40  __a) {
  fract16  __rval = __builtin_bfin_A_madh_FU(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MADH_FU */

#if (!defined(__DEFINED_BFIN_A_MADH) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MADH)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MADH))))

#define __DEFINED_BFIN_A_MADH

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_A_madh(acc40  __a) {
  fract16  __rval = __builtin_bfin_A_madh(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MADH */

#if (!defined(__DEFINED_BFIN_A_MADH_IU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MADH_IU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MADH_IU))))

#define __DEFINED_BFIN_A_MADH_IU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned short  bfin_A_madh_IU(acc40  __a) {
  unsigned short  __rval = __builtin_bfin_A_madh_IU(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MADH_IU */

#if (!defined(__DEFINED_BFIN_A_MADH_IS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MADH_IS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MADH_IS))))

#define __DEFINED_BFIN_A_MADH_IS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_A_madh_IS(acc40  __a) {
  short  __rval = __builtin_bfin_A_madh_IS(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MADH_IS */

#if (!defined(__DEFINED_BFIN_A_MAD_ISS2) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MAD_ISS2)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MAD_ISS2))))

#define __DEFINED_BFIN_A_MAD_ISS2

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_A_mad_ISS2(acc40  __a) {
  int  __rval = __builtin_bfin_A_mad_ISS2(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MAD_ISS2 */

#if (!defined(__DEFINED_BFIN_A_MAD_S2RND) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MAD_S2RND)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MAD_S2RND))))

#define __DEFINED_BFIN_A_MAD_S2RND

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_A_mad_S2RND(acc40  __a) {
  fract32  __rval = __builtin_bfin_A_mad_S2RND(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MAD_S2RND */

#if (!defined(__DEFINED_BFIN_A_MAD_FU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MAD_FU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MAD_FU))))

#define __DEFINED_BFIN_A_MAD_FU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_A_mad_FU(acc40  __a) {
  fract32  __rval = __builtin_bfin_A_mad_FU(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MAD_FU */

#if (!defined(__DEFINED_BFIN_A_MAD) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_MAD)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_MAD))))

#define __DEFINED_BFIN_A_MAD

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_A_mad(acc40  __a) {
  fract32  __rval = __builtin_bfin_A_mad(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_MAD */

#if (!defined(__DEFINED_BFIN_A_SAT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_A_SAT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_A_SAT))))

#define __DEFINED_BFIN_A_SAT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static acc40  bfin_A_sat(acc40  __a) {
  acc40  __rval = __builtin_bfin_A_sat(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_A_SAT */

#if (!defined(__DEFINED_BFIN_EMUCLK) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_EMUCLK)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_EMUCLK))))

#define __DEFINED_BFIN_EMUCLK

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned long long  bfin_emuclk(void) {
  unsigned long long  __rval = __builtin_bfin_emuclk();
  return __rval;
}

#endif /* __DEFINED_BFIN_EMUCLK */

#if (!defined(__DEFINED_BFIN_SYSREG_WRITE) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SYSREG_WRITE)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SYSREG_WRITE))))

#define __DEFINED_BFIN_SYSREG_WRITE

#define bfin_sysreg_write(A, B) (__builtin_bfin_sysreg_write((A), (B)))

#endif /* __DEFINED_BFIN_SYSREG_WRITE */

#if (!defined(__DEFINED_BFIN_SYSREG_READ) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SYSREG_READ)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SYSREG_READ))))

#define __DEFINED_BFIN_SYSREG_READ

#define bfin_sysreg_read(A) (__builtin_bfin_sysreg_read((A)))

#endif /* __DEFINED_BFIN_SYSREG_READ */

#if (!defined(__DEFINED_BFIN_SSYNC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SSYNC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SSYNC))))

#define __DEFINED_BFIN_SSYNC

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_ssync(void) {
  __builtin_bfin_ssync();
}

#endif /* __DEFINED_BFIN_SSYNC */

#if (!defined(__DEFINED_BFIN_CSYNC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CSYNC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CSYNC))))

#define __DEFINED_BFIN_CSYNC

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_csync(void) {
  __builtin_bfin_csync();
}

#endif /* __DEFINED_BFIN_CSYNC */

#if (!defined(__DEFINED_BFIN_SYSREG_WRITE64) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SYSREG_WRITE64)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SYSREG_WRITE64))))

#define __DEFINED_BFIN_SYSREG_WRITE64

#define bfin_sysreg_write64(A, B) (__builtin_bfin_sysreg_write64((A), (B)))

#endif /* __DEFINED_BFIN_SYSREG_WRITE64 */

#if (!defined(__DEFINED_BFIN_SYSREG_READ64) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SYSREG_READ64)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SYSREG_READ64))))

#define __DEFINED_BFIN_SYSREG_READ64

#define bfin_sysreg_read64(A) (__builtin_bfin_sysreg_read64((A)))

#endif /* __DEFINED_BFIN_SYSREG_READ64 */

#if (!defined(__DEFINED_BFIN_UNTESTSET) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_UNTESTSET)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_UNTESTSET))))

#define __DEFINED_BFIN_UNTESTSET

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void  bfin_untestset(char * __a) {
  __builtin_bfin_untestset(__a);
}

#endif /* __DEFINED_BFIN_UNTESTSET */

#if (!defined(__DEFINED_BFIN_BITMUX_SHL_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_BITMUX_SHL_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_BITMUX_SHL_R1))))

#define __DEFINED_BFIN_BITMUX_SHL_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  bfin_bitmux_shl_r1(long long  __a, int  __b, int  __c, int  *__r2, int  *__r3) {
  long long  __rval = __builtin_bfin_bitmux_shl_r1(__a, __b, __c);
  *__r2 = __builtin_bitmux_shl_r2(__rval);
  *__r3 = __builtin_bitmux_shl_r3(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_BITMUX_SHL_R1 */

#if (!defined(__DEFINED_BFIN_BITMUX_SHR_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_BITMUX_SHR_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_BITMUX_SHR_R1))))

#define __DEFINED_BFIN_BITMUX_SHR_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  bfin_bitmux_shr_r1(long long  __a, int  __b, int  __c, int  *__r2, int  *__r3) {
  long long  __rval = __builtin_bfin_bitmux_shr_r1(__a, __b, __c);
  *__r2 = __builtin_bitmux_shr_r2(__rval);
  *__r3 = __builtin_bitmux_shr_r3(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_BITMUX_SHR_R1 */

#if (!defined(__DEFINED_BFIN_BYTEUNPACKR_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_BYTEUNPACKR_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_BYTEUNPACKR_R1))))

#define __DEFINED_BFIN_BYTEUNPACKR_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_byteunpackr_r1(long long  __a, char * __b, int  *__r2) {
  int  __rval = __builtin_bfin_byteunpackr_r1(__a, __b);
  *__r2 = __builtin_byteunpackr_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_BYTEUNPACKR_R1 */

#if (!defined(__DEFINED_BFIN_BYTEUNPACK_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_BYTEUNPACK_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_BYTEUNPACK_R1))))

#define __DEFINED_BFIN_BYTEUNPACK_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_byteunpack_r1(long long  __a, char * __b, int  *__r2) {
  int  __rval = __builtin_bfin_byteunpack_r1(__a, __b);
  *__r2 = __builtin_byteunpack_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_BYTEUNPACK_R1 */

#if (!defined(__DEFINED_BFIN_SAA_R_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SAA_R_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SAA_R_R1))))

#define __DEFINED_BFIN_SAA_R_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_saa_r_r1(long long  __a, char * __b, long long  __c, char * __d, int  __e, int  __f, int  *__r2) {
  int  __rval = __builtin_bfin_saa_r_r1(__a, __b, __c, __d, __e, __f);
  *__r2 = __builtin_saa_r_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_SAA_R_R1 */

#if (!defined(__DEFINED_BFIN_SAA_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SAA_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SAA_R1))))

#define __DEFINED_BFIN_SAA_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_saa_r1(long long  __a, char * __b, long long  __c, char * __d, int  __e, int  __f, int  *__r2) {
  int  __rval = __builtin_bfin_saa_r1(__a, __b, __c, __d, __e, __f);
  *__r2 = __builtin_saa_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_SAA_R1 */

#if (!defined(__DEFINED_BFIN_SUB_I4X8_R_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SUB_I4X8_R_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SUB_I4X8_R_R1))))

#define __DEFINED_BFIN_SUB_I4X8_R_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_sub_i4x8_r_r1(long long  __a, char * __b, long long  __c, char * __d, int  *__r2) {
  int  __rval = __builtin_bfin_sub_i4x8_r_r1(__a, __b, __c, __d);
  *__r2 = __builtin_sub_i4x8_r_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_SUB_I4X8_R_R1 */

#if (!defined(__DEFINED_BFIN_SUB_I4X8_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SUB_I4X8_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SUB_I4X8_R1))))

#define __DEFINED_BFIN_SUB_I4X8_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_sub_i4x8_r1(long long  __a, char * __b, long long  __c, char * __d, int  *__r2) {
  int  __rval = __builtin_bfin_sub_i4x8_r1(__a, __b, __c, __d);
  *__r2 = __builtin_sub_i4x8_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_SUB_I4X8_R1 */

#if (!defined(__DEFINED_BFIN_AVG_I2X8_HITR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I2X8_HITR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I2X8_HITR))))

#define __DEFINED_BFIN_AVG_I2X8_HITR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i2x8_hitr(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_bfin_avg_i2x8_hitr(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I2X8_HITR */

#if (!defined(__DEFINED_BFIN_AVG_I2X8_HIR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I2X8_HIR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I2X8_HIR))))

#define __DEFINED_BFIN_AVG_I2X8_HIR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i2x8_hir(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_bfin_avg_i2x8_hir(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I2X8_HIR */

#if (!defined(__DEFINED_BFIN_AVG_I2X8_HIT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I2X8_HIT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I2X8_HIT))))

#define __DEFINED_BFIN_AVG_I2X8_HIT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i2x8_hit(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_bfin_avg_i2x8_hit(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I2X8_HIT */

#if (!defined(__DEFINED_BFIN_AVG_I2X8_HI) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I2X8_HI)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I2X8_HI))))

#define __DEFINED_BFIN_AVG_I2X8_HI

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i2x8_hi(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_bfin_avg_i2x8_hi(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I2X8_HI */

#if (!defined(__DEFINED_BFIN_AVG_I2X8_LOTR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I2X8_LOTR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I2X8_LOTR))))

#define __DEFINED_BFIN_AVG_I2X8_LOTR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i2x8_lotr(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_bfin_avg_i2x8_lotr(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I2X8_LOTR */

#if (!defined(__DEFINED_BFIN_AVG_I2X8_LOR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I2X8_LOR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I2X8_LOR))))

#define __DEFINED_BFIN_AVG_I2X8_LOR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i2x8_lor(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_bfin_avg_i2x8_lor(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I2X8_LOR */

#if (!defined(__DEFINED_BFIN_AVG_I2X8_LOT) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I2X8_LOT)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I2X8_LOT))))

#define __DEFINED_BFIN_AVG_I2X8_LOT

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i2x8_lot(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_bfin_avg_i2x8_lot(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I2X8_LOT */

#if (!defined(__DEFINED_BFIN_AVG_I2X8_LO) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I2X8_LO)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I2X8_LO))))

#define __DEFINED_BFIN_AVG_I2X8_LO

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i2x8_lo(long long  __a, char * __b, long long  __c) {
  int  __rval = __builtin_bfin_avg_i2x8_lo(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I2X8_LO */

#if (!defined(__DEFINED_BFIN_AVG_I4X8_TR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I4X8_TR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I4X8_TR))))

#define __DEFINED_BFIN_AVG_I4X8_TR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i4x8_tr(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_bfin_avg_i4x8_tr(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I4X8_TR */

#if (!defined(__DEFINED_BFIN_AVG_I4X8_R) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I4X8_R)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I4X8_R))))

#define __DEFINED_BFIN_AVG_I4X8_R

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i4x8_r(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_bfin_avg_i4x8_r(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I4X8_R */

#if (!defined(__DEFINED_BFIN_AVG_I4X8_T) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I4X8_T)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I4X8_T))))

#define __DEFINED_BFIN_AVG_I4X8_T

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i4x8_t(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_bfin_avg_i4x8_t(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I4X8_T */

#if (!defined(__DEFINED_BFIN_AVG_I4X8) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_AVG_I4X8)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_AVG_I4X8))))

#define __DEFINED_BFIN_AVG_I4X8

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_avg_i4x8(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_bfin_avg_i4x8(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_BFIN_AVG_I4X8 */

#if (!defined(__DEFINED_BFIN_ADD_I4X8_R_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ADD_I4X8_R_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ADD_I4X8_R_R1))))

#define __DEFINED_BFIN_ADD_I4X8_R_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_add_i4x8_r_r1(long long  __a, char * __b, long long  __c, char * __d, int  *__r2) {
  int  __rval = __builtin_bfin_add_i4x8_r_r1(__a, __b, __c, __d);
  *__r2 = __builtin_add_i4x8_r_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_ADD_I4X8_R_R1 */

#if (!defined(__DEFINED_BFIN_ADD_I4X8_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ADD_I4X8_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ADD_I4X8_R1))))

#define __DEFINED_BFIN_ADD_I4X8_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_add_i4x8_r1(long long  __a, char * __b, long long  __c, char * __d, int  *__r2) {
  int  __rval = __builtin_bfin_add_i4x8_r1(__a, __b, __c, __d);
  *__r2 = __builtin_add_i4x8_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_ADD_I4X8_R1 */

#if (!defined(__DEFINED_BFIN_EXTRACT_AND_ADD_R1) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_EXTRACT_AND_ADD_R1)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_EXTRACT_AND_ADD_R1))))

#define __DEFINED_BFIN_EXTRACT_AND_ADD_R1

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_extract_and_add_r1(int  __a, int  __b, int  *__r2) {
  int  __rval = __builtin_bfin_extract_and_add_r1(__a, __b);
  *__r2 = __builtin_extract_and_add_r2(__rval);
  return __rval;
}

#endif /* __DEFINED_BFIN_EXTRACT_AND_ADD_R1 */

#if (!defined(__DEFINED_BFIN_ADDCLIP_HIR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ADDCLIP_HIR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ADDCLIP_HIR))))

#define __DEFINED_BFIN_ADDCLIP_HIR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_addclip_hir(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_bfin_addclip_hir(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_BFIN_ADDCLIP_HIR */

#if (!defined(__DEFINED_BFIN_ADDCLIP_LOR) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ADDCLIP_LOR)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ADDCLIP_LOR))))

#define __DEFINED_BFIN_ADDCLIP_LOR

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_addclip_lor(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_bfin_addclip_lor(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_BFIN_ADDCLIP_LOR */

#if (!defined(__DEFINED_BFIN_ADDCLIP_HI) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ADDCLIP_HI)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ADDCLIP_HI))))

#define __DEFINED_BFIN_ADDCLIP_HI

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_addclip_hi(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_bfin_addclip_hi(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_BFIN_ADDCLIP_HI */

#if (!defined(__DEFINED_BFIN_ADDCLIP_LO) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_ADDCLIP_LO)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_ADDCLIP_LO))))

#define __DEFINED_BFIN_ADDCLIP_LO

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_addclip_lo(long long  __a, char * __b, long long  __c, char * __d) {
  int  __rval = __builtin_bfin_addclip_lo(__a, __b, __c, __d);
  return __rval;
}

#endif /* __DEFINED_BFIN_ADDCLIP_LO */

#if (!defined(__DEFINED_BFIN_MULTU64_32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULTU64_32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULTU64_32X32))))

#define __DEFINED_BFIN_MULTU64_32X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned long long  bfin_multu64_32x32(unsigned int  __a, unsigned int  __b) {
  unsigned long long  __rval = __builtin_bfin_multu64_32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULTU64_32X32 */

#if (!defined(__DEFINED_BFIN_MULT64_32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULT64_32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULT64_32X32))))

#define __DEFINED_BFIN_MULT64_32X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  bfin_mult64_32x32(int  __a, int  __b) {
  long long  __rval = __builtin_bfin_mult64_32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULT64_32X32 */

#if (!defined(__DEFINED_BFIN_SHL_FR2X16_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHL_FR2X16_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHL_FR2X16_CLIP))))

#define __DEFINED_BFIN_SHL_FR2X16_CLIP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_shl_fr2x16_clip(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_bfin_shl_fr2x16_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHL_FR2X16_CLIP */

#if (!defined(__DEFINED_BFIN_SHL_FR1X32_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHL_FR1X32_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHL_FR1X32_CLIP))))

#define __DEFINED_BFIN_SHL_FR1X32_CLIP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_shl_fr1x32_clip(fract32  __a, short  __b) {
  fract32  __rval = __builtin_bfin_shl_fr1x32_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHL_FR1X32_CLIP */

#if (!defined(__DEFINED_BFIN_SHRL_FR2X16_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHRL_FR2X16_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHRL_FR2X16_CLIP))))

#define __DEFINED_BFIN_SHRL_FR2X16_CLIP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_shrl_fr2x16_clip(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_bfin_shrl_fr2x16_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHRL_FR2X16_CLIP */

#if (!defined(__DEFINED_BFIN_SHR_FR2X16_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHR_FR2X16_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHR_FR2X16_CLIP))))

#define __DEFINED_BFIN_SHR_FR2X16_CLIP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_shr_fr2x16_clip(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_bfin_shr_fr2x16_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHR_FR2X16_CLIP */

#if (!defined(__DEFINED_BFIN_SHR_FR1X32_CLIP) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHR_FR1X32_CLIP)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHR_FR1X32_CLIP))))

#define __DEFINED_BFIN_SHR_FR1X32_CLIP

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_shr_fr1x32_clip(fract32  __a, short  __b) {
  fract32  __rval = __builtin_bfin_shr_fr1x32_clip(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHR_FR1X32_CLIP */

#if (!defined(__DEFINED_BFIN_SHRL_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHRL_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHRL_FR2X16))))

#define __DEFINED_BFIN_SHRL_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_shrl_fr2x16(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_bfin_shrl_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHRL_FR2X16 */

#if (!defined(__DEFINED_BFIN_SHR_I1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHR_I1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHR_I1X16))))

#define __DEFINED_BFIN_SHR_I1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_shr_i1x16(short  __a, short  __b) {
  short  __rval = __builtin_bfin_shr_i1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHR_I1X16 */

#if (!defined(__DEFINED_BFIN_SHR_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHR_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHR_FR2X16))))

#define __DEFINED_BFIN_SHR_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract2x16  bfin_shr_fr2x16(fract2x16  __a, short  __b) {
  fract2x16  __rval = __builtin_bfin_shr_fr2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHR_FR2X16 */

#if (!defined(__DEFINED_BFIN_SHR_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHR_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHR_FR1X32))))

#define __DEFINED_BFIN_SHR_FR1X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_shr_fr1x32(fract32  __a, short  __b) {
  fract32  __rval = __builtin_bfin_shr_fr1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHR_FR1X32 */

#if (!defined(__DEFINED_BFIN_SHR_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SHR_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SHR_FR1X16))))

#define __DEFINED_BFIN_SHR_FR1X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_shr_fr1x16(fract16  __a, short  __b) {
  fract16  __rval = __builtin_bfin_shr_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_SHR_FR1X16 */

#if (!defined(__DEFINED_BFIN_CMPLX_MUL32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_MUL32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_MUL32))))

#define __DEFINED_BFIN_CMPLX_MUL32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  bfin_cmplx_mul32(long long  __a, long long  __b) {
  long long  __rval = __builtin_bfin_cmplx_mul32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_MUL32 */

#if (!defined(__DEFINED_BFIN_CONJ_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CONJ_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CONJ_FR32))))

#define __DEFINED_BFIN_CONJ_FR32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  bfin_conj_fr32(long long  __a) {
  long long  __rval = __builtin_bfin_conj_fr32(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_CONJ_FR32 */

#if (!defined(__DEFINED_BFIN_CSUB_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CSUB_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CSUB_FR32))))

#define __DEFINED_BFIN_CSUB_FR32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  bfin_csub_fr32(long long  __a, long long  __b) {
  long long  __rval = __builtin_bfin_csub_fr32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_CSUB_FR32 */

#if (!defined(__DEFINED_BFIN_CADD_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CADD_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CADD_FR32))))

#define __DEFINED_BFIN_CADD_FR32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  bfin_cadd_fr32(long long  __a, long long  __b) {
  long long  __rval = __builtin_bfin_cadd_fr32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_CADD_FR32 */

#if (!defined(__DEFINED_BFIN_DIFF_LH_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_DIFF_LH_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_DIFF_LH_FR2X16))))

#define __DEFINED_BFIN_DIFF_LH_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_diff_lh_fr2x16(fract2x16  __a) {
  fract16  __rval = __builtin_bfin_diff_lh_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_DIFF_LH_FR2X16 */

#if (!defined(__DEFINED_BFIN_DIFF_HL_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_DIFF_HL_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_DIFF_HL_FR2X16))))

#define __DEFINED_BFIN_DIFF_HL_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_diff_hl_fr2x16(fract2x16  __a) {
  fract16  __rval = __builtin_bfin_diff_hl_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_DIFF_HL_FR2X16 */

#if (!defined(__DEFINED_BFIN_CSQU_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CSQU_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CSQU_FR16))))

#define __DEFINED_BFIN_CSQU_FR16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_csqu_fr16(int  __a) {
  int  __rval = __builtin_bfin_csqu_fr16(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_CSQU_FR16 */

#if (!defined(__DEFINED_BFIN_CMPLX_CONJ_MUL_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_CONJ_MUL_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_CONJ_MUL_S40))))

#define __DEFINED_BFIN_CMPLX_CONJ_MUL_S40

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_conj_mul_s40(int  __a, int  __b) {
  int  __rval = __builtin_bfin_cmplx_conj_mul_s40(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_CONJ_MUL_S40 */

#if (!defined(__DEFINED_BFIN_CMPLX_CONJ_MSU_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_CONJ_MSU_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_CONJ_MSU_S40))))

#define __DEFINED_BFIN_CMPLX_CONJ_MSU_S40

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_conj_msu_s40(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_bfin_cmplx_conj_msu_s40(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_CONJ_MSU_S40 */

#if (!defined(__DEFINED_BFIN_CMPLX_CONJ_MAC_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_CONJ_MAC_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_CONJ_MAC_S40))))

#define __DEFINED_BFIN_CMPLX_CONJ_MAC_S40

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_conj_mac_s40(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_bfin_cmplx_conj_mac_s40(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_CONJ_MAC_S40 */

#if (!defined(__DEFINED_BFIN_CMPLX_CONJ_MSU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_CONJ_MSU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_CONJ_MSU))))

#define __DEFINED_BFIN_CMPLX_CONJ_MSU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_conj_msu(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_bfin_cmplx_conj_msu(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_CONJ_MSU */

#if (!defined(__DEFINED_BFIN_CMPLX_CONJ_MAC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_CONJ_MAC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_CONJ_MAC))))

#define __DEFINED_BFIN_CMPLX_CONJ_MAC

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_conj_mac(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_bfin_cmplx_conj_mac(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_CONJ_MAC */

#if (!defined(__DEFINED_BFIN_CMPLX_MUL_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_MUL_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_MUL_S40))))

#define __DEFINED_BFIN_CMPLX_MUL_S40

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_mul_s40(int  __a, int  __b) {
  int  __rval = __builtin_bfin_cmplx_mul_s40(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_MUL_S40 */

#if (!defined(__DEFINED_BFIN_CMPLX_MSU_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_MSU_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_MSU_S40))))

#define __DEFINED_BFIN_CMPLX_MSU_S40

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_msu_s40(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_bfin_cmplx_msu_s40(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_MSU_S40 */

#if (!defined(__DEFINED_BFIN_CMPLX_MAC_S40) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_MAC_S40)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_MAC_S40))))

#define __DEFINED_BFIN_CMPLX_MAC_S40

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_mac_s40(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_bfin_cmplx_mac_s40(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_MAC_S40 */

#if (!defined(__DEFINED_BFIN_CMPLX_MSU) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_MSU)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_MSU))))

#define __DEFINED_BFIN_CMPLX_MSU

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_msu(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_bfin_cmplx_msu(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_MSU */

#if (!defined(__DEFINED_BFIN_CMPLX_MAC) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_CMPLX_MAC)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_CMPLX_MAC))))

#define __DEFINED_BFIN_CMPLX_MAC

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_cmplx_mac(int  __a, int  __b, int  __c) {
  int  __rval = __builtin_bfin_cmplx_mac(__a, __b, __c);
  return __rval;
}

#endif /* __DEFINED_BFIN_CMPLX_MAC */

#if (!defined(__DEFINED_BFIN_SUM_I2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SUM_I2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SUM_I2X16))))

#define __DEFINED_BFIN_SUM_I2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_sum_i2x16(int  __a) {
  short  __rval = __builtin_bfin_sum_i2x16(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_SUM_I2X16 */

#if (!defined(__DEFINED_BFIN_BYTESWAP2) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_BYTESWAP2)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_BYTESWAP2))))

#define __DEFINED_BFIN_BYTESWAP2

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  bfin_byteswap2(short  __a) {
  short  __rval = __builtin_bfin_byteswap2(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_BYTESWAP2 */

#if (!defined(__DEFINED_BFIN_BYTESWAP4) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_BYTESWAP4)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_BYTESWAP4))))

#define __DEFINED_BFIN_BYTESWAP4

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  bfin_byteswap4(int  __a) {
  int  __rval = __builtin_bfin_byteswap4(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_BYTESWAP4 */

#if (!defined(__DEFINED_BFIN_MULT_FR1X32X32NS) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULT_FR1X32X32NS)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULT_FR1X32X32NS))))

#define __DEFINED_BFIN_MULT_FR1X32X32NS

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_mult_fr1x32x32NS(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_bfin_mult_fr1x32x32NS(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULT_FR1X32X32NS */

#if (!defined(__DEFINED_BFIN_MULTR_FR1X32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULTR_FR1X32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULTR_FR1X32X32))))

#define __DEFINED_BFIN_MULTR_FR1X32X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_multr_fr1x32x32(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_bfin_multr_fr1x32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULTR_FR1X32X32 */

#if (!defined(__DEFINED_BFIN_MULT_FR1X32X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_MULT_FR1X32X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_MULT_FR1X32X32))))

#define __DEFINED_BFIN_MULT_FR1X32X32

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  bfin_mult_fr1x32x32(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_bfin_mult_fr1x32x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_BFIN_MULT_FR1X32X32 */

#if (!defined(__DEFINED_BFIN_SUM_FR2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_BFIN_SUM_FR2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_BFIN_SUM_FR2X16))))

#define __DEFINED_BFIN_SUM_FR2X16

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  bfin_sum_fr2x16(fract2x16  __a) {
  fract16  __rval = __builtin_bfin_sum_fr2x16(__a);
  return __rval;
}

#endif /* __DEFINED_BFIN_SUM_FR2X16 */

#if (!defined(__DEFINED_SET_RND_MOD_BIASED) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SET_RND_MOD_BIASED)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SET_RND_MOD_BIASED))))

#define __DEFINED_SET_RND_MOD_BIASED

/* Set the RND_MOD bit to enable biased rounding. The result is 
** the previous value of ASTAT which may be passed into the 
** restore_rnd_mod builtin if you wish to restore the state of the 
** RND_MOD bit to its previous value
*/

#define set_rnd_mod_biased() (__builtin_set_rnd_mod_biased())

#endif /* __DEFINED_SET_RND_MOD_BIASED */

#if (!defined(__DEFINED_SET_RND_MOD_UNBIASED) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SET_RND_MOD_UNBIASED)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SET_RND_MOD_UNBIASED))))

#define __DEFINED_SET_RND_MOD_UNBIASED

/* Clear the RND_MOD bit to enable unbiased rounding. The result is 
** the previous value of ASTAT which may be passed into the 
** restore_rnd_mod builtin if you wish to restore the state of the 
** RND_MOD bit to its previous value
*/

#define set_rnd_mod_unbiased() (__builtin_set_rnd_mod_unbiased())

#endif /* __DEFINED_SET_RND_MOD_UNBIASED */

#if (!defined(__DEFINED_RESTORE_RND_MOD) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_RESTORE_RND_MOD)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_RESTORE_RND_MOD))))

#define __DEFINED_RESTORE_RND_MOD

/* Reset the state of the RND_MOD bit to a previous value as saved 
** by the set_rnd_mod_biased or set_rnd_mod_unbiased builtins
*/

#define restore_rnd_mod(A) (__builtin_restore_rnd_mod((A)))

#endif /* __DEFINED_RESTORE_RND_MOD */

#if (!defined(__DEFINED_SAT_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SAT_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SAT_FR1X32))))

#define __DEFINED_SAT_FR1X32

/* If __a > 0x7fff, returns 0x7fff. If __a < -0x8000, returns
** -0x8000. Otherwise returns the lower 16 bits of __a.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  sat_fr1x32(fract32  __a) {
  fract16  __rval = __builtin_sat_fr1x32(__a);
  return __rval;
}

#endif /* __DEFINED_SAT_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SAT_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SAT_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SAT_FX1X32))))

#define __DEFINED_SAT_FX1X32

/* If __a > 0x7fff, returns 0x7fff. If __a < -0x8000, returns
** -0x8000. Otherwise returns the lower 16 bits of __a.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  sat_fx1x32(long _Fract  __a) {
  _Fract  __rval = __builtin_sat_fx1x32(__a);
  return __rval;
}

#endif /* __DEFINED_SAT_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_SAT_FR1X64) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SAT_FR1X64)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SAT_FR1X64))))

#define __DEFINED_SAT_FR1X64

/* If __a > 0x7fffffff, returns 0x7fffffff. 
** If __a < -0x80000000, returns -0x80000000. 
** Otherwise returns the lower 32 bits of __a.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  sat_fr1x64(long long  __a) {
  int  __rval = __builtin_sat_fr1x64(__a);
  return __rval;
}

#endif /* __DEFINED_SAT_FR1X64 */

#if (!defined(__DEFINED_MULT_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_FR1X16))))

#define __DEFINED_MULT_FR1X16

/* 16-bit fractional multiplication with the T option to 
** generate a 16-bit truncated result
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  mult_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_mult_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MULT_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_FX1X16))))

#define __DEFINED_MULT_FX1X16

/* 16-bit fractional multiplication with the T option to 
** generate a 16-bit truncated result
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  mult_fx1x16(_Fract  __a, _Fract  __b) {
  _Fract  __rval = __builtin_mult_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MULTU_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTU_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTU_FR1X16))))

#define __DEFINED_MULTU_FR1X16

/* 16-bit unsigned fractional multiplication using the FU option 
** and generates a 16-bit truncated result
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  multu_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_multu_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTU_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MULTU_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULTU_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULTU_FX1X16))))

#define __DEFINED_MULTU_FX1X16

/* 16-bit unsigned fractional multiplication using the FU option 
** and generates a 16-bit truncated result
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned _Fract  multu_fx1x16(unsigned _Fract  __a, unsigned _Fract  __b) {
  unsigned _Fract  __rval = __builtin_multu_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULTU_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_ADD_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_FR1X16))))

#define __DEFINED_ADD_FR1X16

/* Performs 16-bit addition of the two input parameters
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  add_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_add_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_ADD_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_FX1X16))))

#define __DEFINED_ADD_FX1X16

/* Performs 16-bit addition of the two input parameters
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  add_fx1x16(_Fract  __a, _Fract  __b) {
  _Fract  __rval = __builtin_add_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_SUB_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUB_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUB_FR1X16))))

#define __DEFINED_SUB_FR1X16

/* Performs 16-bit saturating fractional subtraction of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  sub_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_sub_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SUB_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SUB_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUB_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUB_FX1X16))))

#define __DEFINED_SUB_FX1X16

/* Performs 16-bit saturating fractional subtraction of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  sub_fx1x16(_Fract  __a, _Fract  __b) {
  _Fract  __rval = __builtin_sub_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SUB_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MIN_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MIN_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MIN_FR1X16))))

#define __DEFINED_MIN_FR1X16

/* Return minimum value of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  min_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_min_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MIN_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MIN_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MIN_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MIN_FX1X16))))

#define __DEFINED_MIN_FX1X16

/* Return minimum value of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  min_fx1x16(_Fract  __a, _Fract  __b) {
  _Fract  __rval = __builtin_min_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MIN_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MAX_FR1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MAX_FR1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MAX_FR1X16))))

#define __DEFINED_MAX_FR1X16

/* Return maximum value of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  max_fr1x16(fract16  __a, fract16  __b) {
  fract16  __rval = __builtin_max_fr1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MAX_FR1X16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MAX_FX1X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MAX_FX1X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MAX_FX1X16))))

#define __DEFINED_MAX_FX1X16

/* Return maximum value of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  max_fx1x16(_Fract  __a, _Fract  __b) {
  _Fract  __rval = __builtin_max_fx1x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MAX_FX1X16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_ADD_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_FR1X32))))

#define __DEFINED_ADD_FR1X32

/* Returns result of saturating addition of the inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  add_fr1x32(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_add_fr1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_ADD_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_ADD_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_ADD_FX1X32))))

#define __DEFINED_ADD_FX1X32

/* Returns result of saturating addition of the inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  add_fx1x32(long _Fract  __a, long _Fract  __b) {
  long _Fract  __rval = __builtin_add_fx1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_ADD_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_SUB_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUB_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUB_FR1X32))))

#define __DEFINED_SUB_FR1X32

/* Returns result of saturating subtraction of the inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  sub_fr1x32(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_sub_fr1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SUB_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_SUB_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_SUB_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_SUB_FX1X32))))

#define __DEFINED_SUB_FX1X32

/* Returns result of saturating subtraction of the inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  sub_fx1x32(long _Fract  __a, long _Fract  __b) {
  long _Fract  __rval = __builtin_sub_fx1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_SUB_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MULT_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_FR1X32))))

#define __DEFINED_MULT_FR1X32

/* Performs a fractional multiplication on two 16-bit fractions, 
** returning the 32-bit result.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  mult_fr1x32(fract16  __a, fract16  __b) {
  fract32  __rval = __builtin_mult_fr1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MULT_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MULT_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MULT_FX1X32))))

#define __DEFINED_MULT_FX1X32

/* Performs a fractional multiplication on two 16-bit fractions, 
** returning the 32-bit result.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  mult_fx1x32(_Fract  __a, _Fract  __b) {
  long _Fract  __rval = __builtin_mult_fx1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MULT_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MIN_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MIN_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MIN_FR1X32))))

#define __DEFINED_MIN_FR1X32

/* Returns the minimum value of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  min_fr1x32(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_min_fr1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MIN_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MIN_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MIN_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MIN_FX1X32))))

#define __DEFINED_MIN_FX1X32

/* Returns the minimum value of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  min_fx1x32(long _Fract  __a, long _Fract  __b) {
  long _Fract  __rval = __builtin_min_fx1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MIN_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_MAX_FR1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MAX_FR1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MAX_FR1X32))))

#define __DEFINED_MAX_FR1X32

/* Returns the maximum value of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  max_fr1x32(fract32  __a, fract32  __b) {
  fract32  __rval = __builtin_max_fr1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MAX_FR1X32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_MAX_FX1X32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MAX_FX1X32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MAX_FX1X32))))

#define __DEFINED_MAX_FX1X32

/* Returns the maximum value of the two inputs
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  max_fx1x32(long _Fract  __a, long _Fract  __b) {
  long _Fract  __rval = __builtin_max_fx1x32(__a, __b);
  return __rval;
}

#endif /* __DEFINED_MAX_FX1X32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_COMPOSE_2X16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_COMPOSE_2X16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_COMPOSE_2X16))))

#define __DEFINED_COMPOSE_2X16

/* Composes a packed integer from two short inputs.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  compose_2x16(short  __a, short  __b) {
  int  __rval = __builtin_compose_2x16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_COMPOSE_2X16 */

#if (!defined(__DEFINED_CCOMPOSE_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CCOMPOSE_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CCOMPOSE_FR16))))

#define __DEFINED_CCOMPOSE_FR16

/* Composes a complex_fract16 from two fract16 inputs.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  ccompose_fr16(fract16  __a, fract16  __b) {
  int  __rval = __builtin_ccompose_fr16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CCOMPOSE_FR16 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_CCOMPOSE_FX_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_CCOMPOSE_FX_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_CCOMPOSE_FX_FR16))))

#define __DEFINED_CCOMPOSE_FX_FR16

/* Composes a complex_fract16 from two fract inputs.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static int  ccompose_fx_fr16(_Fract  __a, _Fract  __b) {
  int  __rval = __builtin_ccompose_fx_fr16(__a, __b);
  return __rval;
}

#endif /* __DEFINED_CCOMPOSE_FX_FR16 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_COMPOSE_I64) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_COMPOSE_I64)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_COMPOSE_I64))))

#define __DEFINED_COMPOSE_I64

/* Composes a 64-bit packed integer from two int inputs.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long long  compose_i64(int  __a, int  __b) {
  long long  __rval = __builtin_compose_i64(__a, __b);
  return __rval;
}

#endif /* __DEFINED_COMPOSE_I64 */

#if (!defined(__DEFINED_EXTRACT_HI) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EXTRACT_HI)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EXTRACT_HI))))

#define __DEFINED_EXTRACT_HI

/* Extract and return the high half of the 32-bit input
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  extract_hi(int  __a) {
  short  __rval = __builtin_extract_hi(__a);
  return __rval;
}

#endif /* __DEFINED_EXTRACT_HI */

#if (!defined(__DEFINED_IMAG_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_IMAG_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_IMAG_FR16))))

#define __DEFINED_IMAG_FR16

/* Extract the imaginary part of a complex_fract16 input as a fract16
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  imag_fr16(int  __a) {
  fract16  __rval = __builtin_imag_fr16(__a);
  return __rval;
}

#endif /* __DEFINED_IMAG_FR16 */

#if (!defined(__DEFINED_IMAG_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_IMAG_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_IMAG_FR32))))

#define __DEFINED_IMAG_FR32

/* Extract the imaginary part of a complex_fract32 input as a fract32
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  imag_fr32(long long  __a) {
  fract32  __rval = __builtin_imag_fr32(__a);
  return __rval;
}

#endif /* __DEFINED_IMAG_FR32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_IMAG_FX_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_IMAG_FX_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_IMAG_FX_FR16))))

#define __DEFINED_IMAG_FX_FR16

/* Extract the imaginary part of a complex_fract16 input as a fract
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  imag_fx_fr16(int  __a) {
  _Fract  __rval = __builtin_imag_fx_fr16(__a);
  return __rval;
}

#endif /* __DEFINED_IMAG_FX_FR16 */
#endif /* __FIXED_POINT_ALLOWED */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_IMAG_FX_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_IMAG_FX_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_IMAG_FX_FR32))))

#define __DEFINED_IMAG_FX_FR32

/* Extract the imaginary part of a complex_fract32 input as a long fract
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  imag_fx_fr32(long long  __a) {
  long _Fract  __rval = __builtin_imag_fx_fr32(__a);
  return __rval;
}

#endif /* __DEFINED_IMAG_FX_FR32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_EXTRACT_LO) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_EXTRACT_LO)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_EXTRACT_LO))))

#define __DEFINED_EXTRACT_LO

/* Extract and return the low half of the 32-bit input
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static short  extract_lo(int  __a) {
  short  __rval = __builtin_extract_lo(__a);
  return __rval;
}

#endif /* __DEFINED_EXTRACT_LO */

#if (!defined(__DEFINED_REAL_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_REAL_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_REAL_FR16))))

#define __DEFINED_REAL_FR16

/* Extract the real part of a complex_fract16 input as a fract16
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract16  real_fr16(int  __a) {
  fract16  __rval = __builtin_real_fr16(__a);
  return __rval;
}

#endif /* __DEFINED_REAL_FR16 */

#if (!defined(__DEFINED_REAL_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_REAL_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_REAL_FR32))))

#define __DEFINED_REAL_FR32

/* Extract the real part of a complex_fract32 input as a fract32
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static fract32  real_fr32(long long  __a) {
  fract32  __rval = __builtin_real_fr32(__a);
  return __rval;
}

#endif /* __DEFINED_REAL_FR32 */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_REAL_FX_FR16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_REAL_FX_FR16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_REAL_FX_FR16))))

#define __DEFINED_REAL_FX_FR16

/* Extract the real part of a complex_fract16 input as a fract
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static _Fract  real_fx_fr16(int  __a) {
  _Fract  __rval = __builtin_real_fx_fr16(__a);
  return __rval;
}

#endif /* __DEFINED_REAL_FX_FR16 */
#endif /* __FIXED_POINT_ALLOWED */

#ifdef __FIXED_POINT_ALLOWED
#if (!defined(__DEFINED_REAL_FX_FR32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_REAL_FX_FR32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_REAL_FX_FR32))))

#define __DEFINED_REAL_FX_FR32

/* Extract the real part of a complex_fract32 input as a long fract
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static long _Fract  real_fx_fr32(long long  __a) {
  long _Fract  __rval = __builtin_real_fx_fr32(__a);
  return __rval;
}

#endif /* __DEFINED_REAL_FX_FR32 */
#endif /* __FIXED_POINT_ALLOWED */

#if (!defined(__DEFINED_UNTESTSET) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_UNTESTSET)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_UNTESTSET))))

#define __DEFINED_UNTESTSET

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void untestset(char * __a) {
  __builtin_untestset(__a);
}

#endif /* __DEFINED_UNTESTSET */

#if (!defined(__DEFINED_MMR_READ16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MMR_READ16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MMR_READ16))))

#define __DEFINED_MMR_READ16

/* Read from a 16-bit wide Memory Mapped Register(MMR).
** This is useful when not using literal addresses for MMR accesses as
** it enables the compiler to apply any necessary MMR related anomaly
** workarounds.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned short  mmr_read16(volatile void * __a) {
  unsigned short  __rval = __builtin_mmr_read16(__a);
  return __rval;
}

#endif /* __DEFINED_MMR_READ16 */

#if (!defined(__DEFINED_MMR_READ32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MMR_READ32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MMR_READ32))))

#define __DEFINED_MMR_READ32

/* Like mmr_read16 but does a 32-bit wide read instead.
** 
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static unsigned int  mmr_read32(volatile void * __a) {
  unsigned int  __rval = __builtin_mmr_read32(__a);
  return __rval;
}

#endif /* __DEFINED_MMR_READ32 */

#if (!defined(__DEFINED_MMR_WRITE16) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MMR_WRITE16)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MMR_WRITE16))))

#define __DEFINED_MMR_WRITE16

/* Write a value to a 16-bit wide Memory Mapped Register(MMR).
** This is useful when not using literal addresses for MMR accesses as
** it enables the compiler to apply any necessary MMR related anomaly
** workarounds.
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void mmr_write16(volatile void * __a, unsigned short  __b) {
  __builtin_mmr_write16(__a, __b);
}

#endif /* __DEFINED_MMR_WRITE16 */

#if (!defined(__DEFINED_MMR_WRITE32) && \
     ((defined(__SPECIFIC_NAMES) && defined(__ENABLE_MMR_WRITE32)) || \
       (!defined(__SPECIFIC_NAMES) && !defined(__DISABLE_MMR_WRITE32))))

#define __DEFINED_MMR_WRITE32

/* Like mmr_write16 but does a 32-bit wide write instead of 16-bit.
** 
*/

#pragma inline
#pragma always_inline
#pragma source_position_from_call_site
static void mmr_write32(volatile void * __a, unsigned int  __b) {
  __builtin_mmr_write32(__a, __b);
}

#endif /* __DEFINED_MMR_WRITE32 */

#endif /* __NO_BUILTIN */

#ifdef _MISRA_RULES
#pragma diag(pop)
#endif

#ifdef __cplusplus
}
#endif
